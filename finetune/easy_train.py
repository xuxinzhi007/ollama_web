#!/usr/bin/env python3
"""
ğŸ¯ ç®€åŒ–ç‰ˆ LoRA è®­ç»ƒè„šæœ¬
ä¸€ä¸ªæ–‡ä»¶æå®šï¼šæ•°æ®å‡†å¤‡ -> è®­ç»ƒ -> å¯¼å‡ºåˆ° Ollama

ç”¨æ³•ï¼š
  python easy_train.py --name "my-model"
  python easy_train.py --name "linzhi" --data data/train.jsonl
"""

import argparse
import json
import os
import sys
import time
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¯ç”¨ HuggingFace ä¸‹è½½è¿›åº¦æ¡
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"
os.environ["TRANSFORMERS_VERBOSITY"] = "info"


def print_step(step: int, total: int, msg: str):
    """æ‰“å°æ­¥éª¤"""
    print(f"\n{'='*50}")
    print(f"ğŸ“ æ­¥éª¤ {step}/{total}: {msg}")
    print(f"{'='*50}\n")


def check_data(data_path: str) -> dict:
    """æ£€æŸ¥è®­ç»ƒæ•°æ®"""
    path = Path(data_path)
    if not path.exists():
        return {"ok": False, "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {data_path}"}
    
    count = 0
    sample = None
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            if count == 0:
                sample = json.loads(line.strip())
            count += 1
    
    return {"ok": True, "count": count, "sample": sample}


def download_model(model_name: str):
    """ä¸‹è½½æ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰"""
    print(f"ğŸ“¥ æ£€æŸ¥æ¨¡å‹: {model_name}")
    
    try:
        from huggingface_hub import snapshot_download, try_to_load_from_cache
        
        # å…ˆæ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰ç¼“å­˜
        try:
            cache_dir = snapshot_download(
                repo_id=model_name,
                local_files_only=True,  # åªç”¨æœ¬åœ°
            )
            print(f"âœ… ä½¿ç”¨æœ¬åœ°ç¼“å­˜: {cache_dir}")
            return True
        except Exception:
            pass
        
        # æœ¬åœ°æ²¡æœ‰ï¼Œéœ€è¦ä¸‹è½½
        print(f"ğŸ’¡ æœ¬åœ°æ²¡æœ‰ç¼“å­˜ï¼Œå¼€å§‹ä¸‹è½½...")
        print(f"â³ é¦–æ¬¡ä¸‹è½½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...\n")
        
        cache_dir = snapshot_download(
            repo_id=model_name,
            resume_download=True,
        )
        print(f"âœ… æ¨¡å‹å·²ä¸‹è½½åˆ°: {cache_dir}")
        return True
        
    except Exception as e:
        print(f"âš ï¸  æ£€æŸ¥å¤±è´¥ï¼Œç›´æ¥åŠ è½½è¯•è¯•: {e}")
        return False


def train(
    model_name: str,
    data_path: str,
    output_name: str,
    epochs: float = 3.0,
    lora_rank: int = 16,
    learning_rate: float = 1e-4,
):
    """æ‰§è¡Œè®­ç»ƒ"""
    
    # å»¶è¿Ÿå¯¼å…¥
    print("ğŸ“¦ åŠ è½½ä¾èµ–åº“...")
    import torch
    from datasets import load_dataset
    from peft import LoraConfig
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from trl import SFTTrainer
    from trl.trainer.sft_config import SFTConfig
    
    # æ£€æµ‹è®¾å¤‡
    if torch.cuda.is_available():
        device = "cuda"
        dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
        print(f"ğŸ® ä½¿ç”¨ CUDA GPU")
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = "mps"
        dtype = torch.float32  # MPS ç”¨ fp32 æ›´ç¨³å®š
        print(f"ğŸ ä½¿ç”¨ Apple MPS")
    else:
        device = "cpu"
        dtype = torch.float32
        print(f"ğŸ’» ä½¿ç”¨ CPUï¼ˆä¼šæ¯”è¾ƒæ…¢ï¼‰")
    
    # åŠ è½½ tokenizer
    print(f"\nğŸ“ åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print("âœ… Tokenizer åŠ è½½å®Œæˆ")
    
    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ¤– åŠ è½½æ¨¡å‹: {model_name}")
    print("â³ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
    
    model_kwargs = {}
    if device == "cuda":
        model_kwargs["device_map"] = "auto"
        model_kwargs["torch_dtype"] = dtype
    
    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
    
    if device in ("mps", "cpu"):
        model.to(device)
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # LoRA é…ç½®
    lora_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_rank * 2,
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    )
    
    # åŠ è½½æ•°æ®
    print(f"\nğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®: {data_path}")
    ds = load_dataset("json", data_files={"train": data_path})
    print(f"âœ… åŠ è½½äº† {len(ds['train'])} æ¡æ•°æ®")
    
    # æ ¼å¼åŒ–å‡½æ•°
    def formatting_func(example):
        messages = example.get("messages", [])
        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
    
    # è®­ç»ƒé…ç½®
    output_dir = Path(f"out/lora_{output_name}")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ ¹æ®è®¾å¤‡è°ƒæ•´å‚æ•°
    if device == "cuda":
        batch_size, grad_accum, max_len = 4, 2, 512
    elif device == "mps":
        batch_size, grad_accum, max_len = 1, 8, 512
    else:
        batch_size, grad_accum, max_len = 1, 16, 256
    
    sft_config = SFTConfig(
        output_dir=str(output_dir),
        num_train_epochs=epochs,
        learning_rate=learning_rate,
        warmup_ratio=0.1,
        weight_decay=0.01,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=grad_accum,
        logging_steps=10,
        save_steps=500,
        save_total_limit=2,
        fp16=(dtype == torch.float16) and device == "cuda",
        bf16=(dtype == torch.bfloat16) and device == "cuda",
        use_mps_device=(device == "mps"),
        max_seq_length=max_len,
        packing=False,
        report_to=None,
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SFTTrainer(
        model=model,
        args=sft_config,
        train_dataset=ds["train"],
        processing_class=tokenizer,
        formatting_func=formatting_func,
        peft_config=lora_config,
    )
    
    try:
        trainer.model.print_trainable_parameters()
    except:
        pass
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"   è½®æ•°: {epochs}")
    print(f"   å­¦ä¹ ç‡: {learning_rate}")
    print(f"   LoRA rank: {lora_rank}")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size} x {grad_accum} = {batch_size * grad_accum}")
    
    start_time = time.time()
    trainer.train()
    elapsed = time.time() - start_time
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼è€—æ—¶: {int(elapsed//60)}åˆ†{int(elapsed%60)}ç§’")
    
    # ä¿å­˜
    trainer.model.save_pretrained(str(output_dir))
    tokenizer.save_pretrained(str(output_dir))
    
    # åˆå¹¶æ¨¡å‹
    print(f"\nğŸ”€ åˆå¹¶ LoRA åˆ°åŸºç¡€æ¨¡å‹...")
    merged_dir = Path(f"out/merged_{output_name}")
    merged_dir.mkdir(parents=True, exist_ok=True)
    
    merged = trainer.model.merge_and_unload()
    merged.save_pretrained(str(merged_dir), safe_serialization=True)
    tokenizer.save_pretrained(str(merged_dir))
    
    print(f"âœ… åˆå¹¶å®Œæˆ: {merged_dir}")
    
    return str(merged_dir)


def export_to_ollama(merged_dir: str, ollama_name: str, system_prompt: str = None):
    """å¯¼å‡ºåˆ° Ollama"""
    import subprocess
    
    merged_path = Path(merged_dir)
    
    # é»˜è®¤ç³»ç»Ÿæç¤º
    if system_prompt is None:
        # å°è¯•ä»è®­ç»ƒæ•°æ®è¯»å–
        train_file = Path("data/train.jsonl")
        if train_file.exists():
            with open(train_file, 'r', encoding='utf-8') as f:
                data = json.loads(f.readline())
                for msg in data.get("messages", []):
                    if msg.get("role") == "system":
                        system_prompt = msg.get("content", "")
                        break
        
        if not system_prompt:
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"
    
    # åˆ›å»º Modelfile
    modelfile = f'''FROM {merged_path.absolute()}

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER num_ctx 2048

SYSTEM """{system_prompt}"""
'''
    
    modelfile_path = merged_path / "Modelfile"
    with open(modelfile_path, 'w', encoding='utf-8') as f:
        f.write(modelfile)
    
    print(f"ğŸ“ Modelfile å·²åˆ›å»º: {modelfile_path}")
    
    # å¯¼å…¥åˆ° Ollama
    print(f"\nğŸ“¦ å¯¼å…¥åˆ° Ollama: {ollama_name}")
    result = subprocess.run(
        f"ollama create {ollama_name} -f {modelfile_path}",
        shell=True,
        capture_output=True,
        text=True
    )
    
    if result.returncode == 0:
        print(f"âœ… å¯¼å…¥æˆåŠŸï¼")
        print(f"\nğŸ‰ å®Œæˆï¼è¿è¡Œä»¥ä¸‹å‘½ä»¤æµ‹è¯•ï¼š")
        print(f"   ollama run {ollama_name}")
        return True
    else:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {result.stderr}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description="ğŸ¯ ç®€åŒ–ç‰ˆ LoRA è®­ç»ƒ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python easy_train.py --name linzhi
  python easy_train.py --name linzhi --data data/train.jsonl --epochs 5
  python easy_train.py --name linzhi --model Qwen/Qwen2.5-0.5B
        """
    )
    
    parser.add_argument("--name", required=True, help="æ¨¡å‹åç§°ï¼ˆç”¨äº Ollamaï¼‰")
    parser.add_argument("--data", default="datasets/linzhi/train.jsonl", help="è®­ç»ƒæ•°æ®è·¯å¾„")
    parser.add_argument("--model", default="Qwen/Qwen2.5-0.5B", help="åŸºç¡€æ¨¡å‹")
    parser.add_argument("--epochs", type=float, default=3.0, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--rank", type=int, default=16, help="LoRA rank")
    parser.add_argument("--lr", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--skip-train", action="store_true", help="è·³è¿‡è®­ç»ƒï¼Œç›´æ¥å¯¼å‡º")
    
    args = parser.parse_args()
    
    print("ğŸ¯ ç®€åŒ–ç‰ˆ LoRA è®­ç»ƒ")
    print("=" * 50)
    print(f"ğŸ“¦ æ¨¡å‹åç§°: {args.name}")
    print(f"ğŸ¤– åŸºç¡€æ¨¡å‹: {args.model}")
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {args.data}")
    print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {args.epochs}")
    print("=" * 50)
    
    total_steps = 4
    
    # æ­¥éª¤ 1: æ£€æŸ¥æ•°æ®
    print_step(1, total_steps, "æ£€æŸ¥è®­ç»ƒæ•°æ®")
    data_info = check_data(args.data)
    if not data_info["ok"]:
        print(f"âŒ {data_info['error']}")
        print(f"ğŸ’¡ è¯·å…ˆå‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œæˆ–è¿è¡Œ: python generate_linzhi_data.py")
        sys.exit(1)
    print(f"âœ… æ‰¾åˆ° {data_info['count']} æ¡è®­ç»ƒæ•°æ®")
    
    merged_dir = f"out/merged_{args.name}"
    
    if not args.skip_train:
        # æ­¥éª¤ 2: ä¸‹è½½æ¨¡å‹
        print_step(2, total_steps, "ä¸‹è½½/åŠ è½½æ¨¡å‹")
        download_model(args.model)
        
        # æ­¥éª¤ 3: è®­ç»ƒ
        print_step(3, total_steps, "LoRA è®­ç»ƒ")
        merged_dir = train(
            model_name=args.model,
            data_path=args.data,
            output_name=args.name,
            epochs=args.epochs,
            lora_rank=args.rank,
            learning_rate=args.lr,
        )
    else:
        print_step(2, total_steps, "è·³è¿‡ä¸‹è½½")
        print_step(3, total_steps, "è·³è¿‡è®­ç»ƒ")
        if not Path(merged_dir).exists():
            print(f"âŒ åˆå¹¶æ¨¡å‹ä¸å­˜åœ¨: {merged_dir}")
            sys.exit(1)
    
    # æ­¥éª¤ 4: å¯¼å‡ºåˆ° Ollama
    print_step(4, total_steps, "å¯¼å‡ºåˆ° Ollama")
    export_to_ollama(merged_dir, args.name)
    
    print("\n" + "=" * 50)
    print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    print("=" * 50)


if __name__ == "__main__":
    main()
