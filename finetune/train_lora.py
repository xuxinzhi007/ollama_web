from __future__ import annotations

import argparse
import os
from dataclasses import asdict
from pathlib import Path
from typing import Any, Dict, List

from env_detect import lora_target_modules_for_qwen, plan_environment, pretty_env_summary
from download_progress import progress_indicator


def _require(pkg: str):
    try:
        __import__(pkg)
    except Exception as e:
        raise RuntimeError(
            f"ç¼ºå°‘ä¾èµ– {pkg}ã€‚è¯·å…ˆå®‰è£…: pip install -r requirements.txtã€‚åŸå§‹é”™è¯¯: {type(e).__name__}: {e}"
        ) from e


def parse_args() -> argparse.Namespace:
    ap = argparse.ArgumentParser()
    ap.add_argument("--model_name_or_path", type=str, default="Qwen/Qwen2.5-0.5B-Instruct")
    ap.add_argument("--train_jsonl", type=str, default="data/train.jsonl")
    ap.add_argument("--val_jsonl", type=str, default="data/val.jsonl")
    ap.add_argument("--output_dir", type=str, default="out/lora")
    ap.add_argument("--merged_dir", type=str, default="out/merged")

    ap.add_argument("--num_train_epochs", type=float, default=2.0)
    ap.add_argument("--learning_rate", type=float, default=2e-4)
    ap.add_argument("--warmup_ratio", type=float, default=0.03)
    ap.add_argument("--weight_decay", type=float, default=0.0)
    ap.add_argument("--logging_steps", type=int, default=10)
    ap.add_argument("--save_steps", type=int, default=200)
    ap.add_argument("--eval_steps", type=int, default=200)
    ap.add_argument("--seed", type=int, default=42)

    ap.add_argument("--max_seq_length", type=int, default=0, help="0 è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©")
    ap.add_argument("--per_device_train_batch_size", type=int, default=0, help="0 è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©")
    ap.add_argument("--gradient_accumulation_steps", type=int, default=0, help="0 è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©")

    ap.add_argument("--lora_r", type=int, default=8)
    ap.add_argument("--lora_alpha", type=int, default=16)
    ap.add_argument("--lora_dropout", type=float, default=0.05)
    ap.add_argument("--target_modules", type=str, default="")  # comma-separated

    ap.add_argument("--merge_and_save", action="store_true", help="è®­ç»ƒå®Œæˆååˆå¹¶ LoRA åˆ° base å¹¶ä¿å­˜åˆ° merged_dir")
    ap.add_argument("--no_eval", action="store_true")
    ap.add_argument("--gradient_checkpointing", action="store_true")
    ap.add_argument("--report_to", type=str, default="none", help="none|tensorboard|wandb ç­‰")
    ap.add_argument("--resume_from_checkpoint", type=str, help="ä»æŒ‡å®šæ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ")

    return ap.parse_args()


def main() -> None:
    args = parse_args()

    # å»¶è¿Ÿå¯¼å…¥ï¼Œæ–¹ä¾¿åœ¨æ²¡è£…ä¾èµ–æ—¶ç»™æ›´å‹å¥½çš„é”™è¯¯
    _require("torch")
    _require("datasets")
    _require("transformers")
    _require("peft")
    _require("trl")

    import torch
    from datasets import load_dataset
    from peft import LoraConfig
    from transformers import AutoModelForCausalLM, AutoTokenizer

    try:
        from trl import SFTTrainer
    except Exception as e:
        raise RuntimeError(f"å¯¼å…¥ trl.SFTTrainer å¤±è´¥ï¼Œè¯·æ£€æŸ¥ trl ç‰ˆæœ¬ã€‚{type(e).__name__}: {e}") from e
    try:
        from trl.trainer.sft_config import SFTConfig
    except Exception as e:
        raise RuntimeError(f"å¯¼å…¥ trl.trainer.sft_config.SFTConfig å¤±è´¥ã€‚{type(e).__name__}: {e}") from e

    overrides: Dict[str, Any] = {}
    if args.max_seq_length:
        overrides["max_seq_length"] = args.max_seq_length
    if args.per_device_train_batch_size:
        overrides["per_device_train_batch_size"] = args.per_device_train_batch_size
    if args.gradient_accumulation_steps:
        overrides["gradient_accumulation_steps"] = args.gradient_accumulation_steps

    plan = plan_environment(overrides=overrides)
    print("[env]", pretty_env_summary(plan))

    # CUDA ä¸€äº›å¸¸è§åŠ é€Ÿå¼€å…³ï¼ˆå®‰å…¨ï¼‰
    if plan.device == "cuda":
        try:
            torch.backends.cuda.matmul.allow_tf32 = True
        except Exception:
            pass

    # dtype
    # æ³¨æ„ï¼šæŸäº› accelerate/transformers ç‰ˆæœ¬åœ¨ MPS ä¸Šä¸å…è®¸ fp16 mixed precisionï¼ˆä¼šæŠ¥é”™ï¼‰ï¼Œ
    # æ‰€ä»¥ env_detect å·²é»˜è®¤æŠŠ MPS è®¾ä¸º fp32ï¼›è¿™é‡Œå†åšä¸€æ¬¡å…œåº•ã€‚
    torch_dtype = {"bf16": torch.bfloat16, "fp16": torch.float16, "fp32": torch.float32}[plan.dtype]

    # tokenizer & model - æ™ºèƒ½ç¼“å­˜æ£€æµ‹
    try:
        from model_cache import smart_model_load_message
        smart_model_load_message(args.model_name_or_path)
    except ImportError:
        print(f"\nğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹: {args.model_name_or_path}")
        print(f"ğŸ’¡ å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡ä½¿ç”¨ï¼Œéœ€è¦ä»ç½‘ç»œä¸‹è½½ï¼ˆçº¦500MB-1GBï¼‰")

    # åŠ è½½tokenizerï¼Œç®€åŒ–æç¤º
    print("â³ åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)
    print("âœ… Tokenizer åŠ è½½å®Œæˆ")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # device_map ç­–ç•¥ï¼šcuda ç”¨ autoï¼›mps/cpu ç›´æ¥æœ¬åœ°åŠ è½½å .to(device)
    device_map = "auto" if plan.device == "cuda" else None

    model_kwargs: Dict[str, Any] = {"device_map": device_map}
    if plan.device != "cpu":
        model_kwargs["dtype"] = torch_dtype

    # åŠ è½½æ¨¡å‹ï¼Œç®€åŒ–æç¤º
    print("â³ åŠ è½½æ¨¡å‹æƒé‡ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, **model_kwargs)
    print("âœ… æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")

    if plan.device in ("mps", "cpu"):
        model.to(plan.device)

    if args.gradient_checkpointing:
        model.gradient_checkpointing_enable()
        model.config.use_cache = False

    # LoRA
    if args.target_modules.strip():
        target_modules = tuple(x.strip() for x in args.target_modules.split(",") if x.strip())
    else:
        target_modules = lora_target_modules_for_qwen()

    lora_cfg = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=list(target_modules),
    )

    # dataset
    train_path = str(Path(args.train_jsonl))
    val_path = str(Path(args.val_jsonl))
    data_files = {"train": train_path}
    if not args.no_eval and Path(val_path).exists():
        data_files["validation"] = val_path

    ds = load_dataset("json", data_files=data_files)

    def formatting_func(example: Dict[str, Any]) -> str:
        messages: List[Dict[str, str]] = example.get("messages") or []
        if not messages:
            # å…¼å®¹ instruction/input/output çš„ç®€æ˜“æ ¼å¼ï¼ˆå¦‚æœç”¨æˆ·æœªæ¥æ¢æ•°æ®ï¼‰
            inst = (example.get("instruction") or "").strip()
            inp = (example.get("input") or "").strip()
            out = (example.get("output") or "").strip()
            user = inst + ("\n\n" + inp if inp else "")
            messages = [{"role": "user", "content": user}, {"role": "assistant", "content": out}]

        # å¯¹äºQwenæ¨¡å‹ï¼Œç¡®ä¿systemæ¶ˆæ¯è¢«æ­£ç¡®å¤„ç†
        # æ£€æŸ¥æ˜¯å¦åŒ…å«systemæ¶ˆæ¯
        has_system = any(msg.get("role") == "system" for msg in messages)

        try:
            # å°è¯•ä½¿ç”¨chat template
            formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)

            # éªŒè¯systemæ¶ˆæ¯æ˜¯å¦è¢«åŒ…å«ï¼ˆç®€å•æ£€æŸ¥ï¼‰
            if has_system:
                system_content = next(msg["content"] for msg in messages if msg.get("role") == "system")
                if system_content[:50] not in formatted:
                    print(f"âš ï¸  è­¦å‘Šï¼šsystemæ¶ˆæ¯å¯èƒ½æœªè¢«æ­£ç¡®å¤„ç†")

            return formatted
        except Exception as e:
            print(f"âš ï¸  Chat templateå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æ ¼å¼: {e}")
            # å›é€€ï¼šæ‰‹åŠ¨æ„å»ºå¯¹è¯æ ¼å¼
            result = ""
            for msg in messages:
                role = msg.get("role", "")
                content = msg.get("content", "")
                if role == "system":
                    result += f"<|system|>\n{content}\n"
                elif role == "user":
                    result += f"<|user|>\n{content}\n"
                elif role == "assistant":
                    result += f"<|assistant|>\n{content}\n"
            return result

    # training args
    per_device_bs = int(plan.defaults["per_device_train_batch_size"])
    grad_accum = int(plan.defaults["gradient_accumulation_steps"])
    max_seq_len = int(plan.defaults["max_seq_length"])

    out_dir = Path(args.output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    eval_strategy = "no" if args.no_eval else "steps"
    report_to = None if args.report_to == "none" else [args.report_to]

    # TRL 0.15.xï¼šé€šè¿‡ SFTConfig ä¼ é€’ max_seq_length/packing ç­‰å‚æ•°
    use_mps_device = plan.device == "mps"
    sft_args = SFTConfig(
        output_dir=str(out_dir),
        num_train_epochs=args.num_train_epochs,
        learning_rate=args.learning_rate,
        warmup_ratio=args.warmup_ratio,
        weight_decay=args.weight_decay,
        per_device_train_batch_size=per_device_bs,
        gradient_accumulation_steps=grad_accum,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        eval_strategy=eval_strategy,
        eval_steps=args.eval_steps if not args.no_eval else None,
        save_total_limit=2,
        lr_scheduler_type="cosine",
        optim="adamw_torch",
        # accelerate å¯¹ MPS çš„ mixed precision æ”¯æŒåœ¨ä¸åŒç‰ˆæœ¬é‡Œå·®å¼‚è¾ƒå¤§ï¼›ä¸ºç¨³å®šèµ·è§ï¼ŒMPS å¼ºåˆ¶å…³é—­ fp16/bf16
        fp16=(plan.dtype == "fp16") and not use_mps_device,
        bf16=(plan.dtype == "bf16") and not use_mps_device,
        use_mps_device=use_mps_device,
        report_to=report_to,
        seed=args.seed,
        dataloader_pin_memory=(plan.device == "cuda"),
        max_seq_length=max_seq_len,
        packing=False,
        resume_from_checkpoint=args.resume_from_checkpoint,
    )

    # å¦‚æœè¦ä»checkpointæ¢å¤ï¼Œéœ€è¦å…ˆåŠ è½½LoRAæƒé‡
    if args.resume_from_checkpoint:
        print(f"ğŸ”„ å‡†å¤‡ä»checkpointæ¢å¤: {args.resume_from_checkpoint}")
        checkpoint_path = Path(args.resume_from_checkpoint)
        if checkpoint_path.exists():
            # æ£€æŸ¥checkpointæ˜¯å¦åŒ…å«LoRAæƒé‡
            adapter_files = list(checkpoint_path.glob("adapter_model.*"))
            if adapter_files:
                print(f"âœ… æ‰¾åˆ°LoRAæƒé‡æ–‡ä»¶: {adapter_files[0].name}")
            else:
                print(f"âš ï¸  è­¦å‘Šï¼šcheckpointä¸­æœªæ‰¾åˆ°LoRAæƒé‡æ–‡ä»¶")
                print(f"   å¯èƒ½æ— æ³•æ­£ç¡®æ¢å¤è®­ç»ƒçŠ¶æ€")
    
    trainer = SFTTrainer(
        model=model,
        args=sft_args,
        train_dataset=ds["train"],
        eval_dataset=None if args.no_eval or "validation" not in ds else ds["validation"],
        processing_class=tokenizer,
        formatting_func=formatting_func,
        peft_config=lora_cfg,
    )
    try:
        trainer.model.print_trainable_parameters()
    except Exception:
        pass

    # è®­ç»ƒï¼ˆä¼šè‡ªåŠ¨å¤„ç†resume_from_checkpointï¼‰
    if args.resume_from_checkpoint:
        print(f"ğŸ”„ å¼€å§‹ä»checkpointæ¢å¤è®­ç»ƒ...")
        print(f"   å¦‚æœlossä»åˆå§‹å€¼å¼€å§‹ï¼Œè¯´æ˜checkpointå¯èƒ½æ²¡æœ‰æ­£ç¡®åŠ è½½")
    
    trainer.train()

    # ä¿å­˜ LoRA adapter
    trainer.model.save_pretrained(str(out_dir))
    tokenizer.save_pretrained(str(out_dir))

    # è®°å½•ç¯å¢ƒ/è¶…å‚
    meta = {
        "env_plan": asdict(plan),
        "args": vars(args),
        "resolved": {"per_device_train_batch_size": per_device_bs, "gradient_accumulation_steps": grad_accum, "max_seq_length": max_seq_len},
    }
    (out_dir / "run_meta.json").write_text(__import__("json").dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")

    if args.merge_and_save:
        merged_dir = Path(args.merged_dir)
        merged_dir.mkdir(parents=True, exist_ok=True)
        merged = trainer.model.merge_and_unload()
        merged.save_pretrained(str(merged_dir), safe_serialization=True)
        tokenizer.save_pretrained(str(merged_dir))
        (merged_dir / "run_meta.json").write_text(__import__("json").dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")

    print(f"å®Œæˆï¼šLoRA è¾“å‡º -> {out_dir}")
    if args.merge_and_save:
        print(f"å®Œæˆï¼šMerged è¾“å‡º -> {args.merged_dir}")


if __name__ == "__main__":
    # è®© Windows æ§åˆ¶å°è¾“å‡º UTF-8 æ›´ç¨³ä¸€ç‚¹
    try:
        os.environ.setdefault("PYTHONUTF8", "1")
    except Exception:
        pass
    main()


