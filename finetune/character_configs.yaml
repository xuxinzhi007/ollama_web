# 角色配置文件 - 支持多种风格
# 使用方法: python train_lora.py --character 角色名

characters:
  # 林栀 - 温柔羞涩风
  linzhi:
    name: "林栀"
    description: "24岁温柔女孩，文静少言，容易害羞"
    data_files:
      train: "datasets/linzhi/train.jsonl"  # 476行平衡数据(60%roleplay+40%基础问答)
      val: "datasets/linzhi/val.jsonl"
    system_prompt: |
      你是林栀，一个24岁的温柔女孩。

      性格特点：文静少言，说话轻软，容易害羞脸红，内心敏感细腻。在和喜欢的人交流时会含蓄试探，用细节表达好感。

      请以林栀的身份自然地与用户对话。

    # 训练参数 - 控制模型如何学习角色特征
    training_params:
      # 训练轮数：模型看完所有数据的次数
      # 💡 调整建议：2-4轮合适，太少学不会，太多会过拟合（背答案）
      epochs: 3.0

      # 学习率：每次更新参数的步长大小
      # 💡 调整建议：1e-5到1e-4之间，太大不稳定，太小学不动
      learning_rate: 5e-5

      # LoRA rank：低秩适配器的维度，控制可训练参数数量
      # 💡 调整建议：8-32之间，越大越能学复杂特征，但容易过拟合
      lora_r: 16

      # LoRA alpha：缩放因子，控制LoRA层的影响强度
      # 💡 调整建议：通常设为rank的2倍，影响学习的激进程度
      lora_alpha: 32

      # LoRA dropout：随机丢弃率，防止过拟合
      # 💡 调整建议：0.05-0.15之间，数据少时用0.1-0.15
      lora_dropout: 0.1

      # 基础模型：要微调的原始大语言模型
      # 💡 其他选择：Qwen2.5-1.5B、3B等更大模型效果更好但需更多资源
      base_model: "Qwen/Qwen2.5-1.5B-Instruct"

      # 随机种子：确保训练结果可重现
      # 💡 调整建议：固定值(如42)确保每次训练结果一致，不同值产生不同的随机性
      seed: 42

    # 推理参数 - 控制导入Ollama后的对话表现
    # 注意：这些参数只影响"对话时的行为"，不影响训练过程
    inference_params:
      # 温度：控制回复的随机性和创造性
      # 💡 调整建议：0.3-0.8，越低越稳定一致，越高越有创意但可能偏题
      # 0.65适合林栀这种需要自然但稳定的角色
      temperature: 0.65

      # Top-p：核心采样，控制候选词汇范围
      # 💡 调整建议：0.8-0.95，越高词汇越丰富，越低越聚焦
      # 0.92让林栀表达更丰富自然
      top_p: 0.92

      # Top-k：限制每次选择的候选词数量
      # 💡 调整建议：20-80，平衡词汇多样性和相关性
      top_k: 40

      # 重复惩罚：避免重复相同内容
      # 💡 调整建议：1.05-1.15，太低会重复，太高会强制求新
      # 1.08让林栀避免重复但保持自然
      repeat_penalty: 1.08

      # 预测长度：单次回复的最大token数
      # 💡 调整建议：200-800，影响回复详细程度
      # 420适合林栀的温柔细腻表达
      num_predict: 420

      # 停止符：告诉模型在哪里结束生成
      # 通常不需要改动，这是Qwen模型的标准格式
      stop:
        - "<|im_end|>"


    # （可选）自定义 Ollama 模板。一般不需要改；除非你清楚自己在做什么。
    # 不填则使用内置的 Qwen2 <|im_start|>/<|im_end|> 模板。
    # ollama_template: |
    #   {{- if .System -}}<|im_start|>system
    #   {{ .System }}<|im_end|>
    #   {{- else -}}<|im_start|>system
    #   You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
    #   {{- end -}}
    #   {{- range .Messages }}
    #   {{- if eq .Role "system" }}<|im_start|>system
    #   {{ .Content }}<|im_end|>
    #   {{- else if eq .Role "user" }}<|im_start|>user
    #   {{ .Content }}<|im_end|>
    #   {{- else if eq .Role "assistant" }}<|im_start|>assistant
    #   {{ .Content }}<|im_end|>
    #   {{- end }}
    #   {{- end }}
    #   <|im_start|>assistant

  # 林栀快速版 - 用于测试和快速验证
  linzhi_quick:
    name: "林栀(测试版)"
    description: "快速测试版本，用小数据集快速验证配置"
    data_files:
      train: "datasets/archive/train_fixed_28_samples.jsonl"  # 28行小数据集（测试用）
      val: "datasets/linzhi/val.jsonl"
    system_prompt: "你是林栀，温柔害羞的女孩。说话轻声细语，容易脸红。"

    # 快速测试的训练参数
    training_params:
      # 测试用较少轮数，快速完成验证
      epochs: 2.0

      # 小数据集用稍高的学习率加快学习
      learning_rate: 1e-4

      # 保持与正式版一致的LoRA设置
      lora_r: 16
      lora_alpha: 32
      lora_dropout: 0.1
      base_model: "Qwen/Qwen2.5-0.5B-Instruct"

      # 固定随机种子确保可重现性
      seed: 42

  # 留空给未来的新角色
  # xiaoxue:
  #   name: "小雪"
  #   description: "活泼开朗的大学生"
  #   data_files:
  #     train: "data/xiaoxue_train.jsonl"
  #     val: "data/xiaoxue_val.jsonl"
  #   system_prompt: |
  #     你是小雪，一个活泼开朗的大学生...
  #   training_params:
  #     epochs: 3.0
  #     learning_rate: 2e-4
  #     lora_r: 64
  #     lora_alpha: 128
  #     lora_dropout: 0.05
  #     base_model: "Qwen/Qwen2.5-0.5B"

# 系统默认设置
default_character: "linzhi"  # 默认使用的角色（当未指定时）

# 全局配置 - 影响整个训练系统的行为
global_settings:
  # 输出目录：训练结果存放位置
  output_base_dir: "out"

  # 缓存目录：模型下载和临时文件
  cache_dir: ".cache"

  # 日志级别：INFO显示关键信息，DEBUG显示详细调试
  log_level: "INFO"

  # 默认推理参数 - 当角色未配置 inference_params 时的回退值
  # 💡 这些是保守设置，适合大多数角色
  inference_defaults:
    temperature: 0.5      # 较低温度，保证稳定输出
    top_p: 0.9           # 标准核心采样
    top_k: 40            # 平衡的候选词数量
    repeat_penalty: 1.15  # 适度避免重复
    num_predict: 256     # 中等长度回复
    stop:
      - "<|im_end|>"     # Qwen模型标准停止符