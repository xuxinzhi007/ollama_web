#!/usr/bin/env python3
"""
é…ç½®ç®¡ç†å·¥å…· - è¯»å–å’Œç®¡ç†è®­ç»ƒå‚æ•°
"""

import yaml
import json
from pathlib import Path
from typing import Dict, Any

class ConfigManager:
    def __init__(self, config_path: str = "config.yaml", character: str = None):
        self.config_path = Path(config_path)
        self.character = character
        self.character_config_path = Path("character_configs.yaml")
        self.config = self.load_config()

    def load_config(self) -> Dict[str, Any]:
        """æ™ºèƒ½åŠ è½½é…ç½®æ–‡ä»¶ - æ”¯æŒè§’è‰²é…ç½®å’Œä¼ ç»Ÿé…ç½®"""

        # ä¼˜å…ˆå°è¯•ä»è§’è‰²é…ç½®è¯»å–
        if self.character and self.character_config_path.exists():
            try:
                return self._load_from_character_config()
            except Exception as e:
                print(f"âš ï¸  ä»è§’è‰²é…ç½®åŠ è½½å¤±è´¥: {e}")
                print("ğŸ’¡ å›é€€åˆ°ä¼ ç»Ÿé…ç½®")

        # å›é€€åˆ°ä¼ ç»Ÿ config.yaml æ–¹å¼
        if not self.config_path.exists():
            print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
            print("ğŸ’¡ ä½¿ç”¨é»˜è®¤é…ç½®")
            return self.get_default_config()

        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_path}")
            return config
        except Exception as e:
            print(f"âŒ é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            print("ğŸ’¡ ä½¿ç”¨é»˜è®¤é…ç½®")
            return self.get_default_config()

    def get_default_config(self) -> Dict[str, Any]:
        """é»˜è®¤é…ç½®"""
        return {
            "model": {
                "base_model": "Qwen/Qwen2.5-0.5B-Instruct",
                "model_type": "qwen"
            },
            "training": {
                "epochs": 2.0,
                "learning_rate": 2e-4,
                "warmup_ratio": 0.03,
                "weight_decay": 0.0,
                "seed": 42
            },
            "lora": {
                "rank": 8,
                "alpha": 16,
                "dropout": 0.05
            },
            "data": {
                "max_seq_length": 0,
                "batch_size": 0,
                "gradient_accumulation": 0
            },
            "logging": {
                "logging_steps": 10,
                "save_steps": 200,
                "eval_steps": 200
            },
            "ollama": {
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 40,
                "repeat_penalty": 1.05,
                "context_length": 4096
            },
            "advanced": {
                "gradient_checkpointing": False,
                "no_eval": False,
                "report_to": "none"
            }
        }

    def _load_from_character_config(self) -> Dict[str, Any]:
        """ä»è§’è‰²é…ç½®æ–‡ä»¶åŠ è½½å¹¶è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼"""
        print(f"ğŸ“‹ ä»è§’è‰²é…ç½®åŠ è½½: {self.character}")

        with open(self.character_config_path, 'r', encoding='utf-8') as f:
            char_configs = yaml.safe_load(f)

        if 'characters' not in char_configs or self.character not in char_configs['characters']:
            raise ValueError(f"è§’è‰² '{self.character}' åœ¨é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°")

        char_config = char_configs['characters'][self.character]
        training_params = char_config.get('training_params', {})

        # å°†è§’è‰²é…ç½®è½¬æ¢ä¸ºæ ‡å‡† config.yaml æ ¼å¼
        standard_config = {
            "model": {
                "base_model": training_params.get('base_model', "Qwen/Qwen2.5-0.5B-Instruct"),
                "model_type": "qwen"
            },
            "training": {
                "epochs": training_params.get('epochs', 2.0),
                "learning_rate": training_params.get('learning_rate', 2e-4),
                "warmup_ratio": 0.03,  # ä½¿ç”¨é»˜è®¤å€¼
                "weight_decay": 0.0,   # ä½¿ç”¨é»˜è®¤å€¼
                "seed": 42             # ä½¿ç”¨é»˜è®¤å€¼
            },
            "lora": {
                "rank": training_params.get('lora_r', 8),      # æ³¨æ„ï¼šè§’è‰²é…ç½®ç”¨ lora_r
                "alpha": training_params.get('lora_alpha', 16), # è§’è‰²é…ç½®ç”¨ lora_alpha
                "dropout": training_params.get('lora_dropout', 0.05) # è§’è‰²é…ç½®ç”¨ lora_dropout
            },
            "data": {
                "max_seq_length": 0,
                "batch_size": 0,
                "gradient_accumulation": 0
            },
            "logging": {
                "logging_steps": 10,
                "save_steps": 200,
                "eval_steps": 200
            },
            "ollama": {
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 40,
                "repeat_penalty": 1.05,
                "context_length": 4096
            },
            "advanced": {
                "gradient_checkpointing": False,
                "no_eval": False,
                "report_to": "none"
            }
        }

        # å¦‚æœè§’è‰²é…ç½®æœ‰æ¨ç†å‚æ•°ï¼Œè¦†ç›– ollama é…ç½®
        inference_params = char_config.get('inference_params', {})
        if inference_params:
            standard_config["ollama"].update({
                "temperature": inference_params.get('temperature', 0.7),
                "top_p": inference_params.get('top_p', 0.9),
                "top_k": inference_params.get('top_k', 40),
                "repeat_penalty": inference_params.get('repeat_penalty', 1.05),
                "context_length": inference_params.get('num_predict', 4096)
            })

        print(f"âœ… è§’è‰²é…ç½®è½¬æ¢æˆåŠŸ: {self.character}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {standard_config['model']['base_model']}")
        print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {standard_config['training']['epochs']}")
        print(f"ğŸ”§ LoRA rank: {standard_config['lora']['rank']}")

        return standard_config

    def get(self, key_path: str, default=None):
        """è·å–é…ç½®å€¼ (æ”¯æŒåµŒå¥—è·¯å¾„ï¼Œå¦‚ 'model.base_model')"""
        keys = key_path.split('.')
        value = self.config

        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default

        return value

    def get_training_args(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒå‚æ•° (ç”¨äº train_lora.py)"""
        return {
            "model_name_or_path": self.get("model.base_model"),
            "num_train_epochs": self.get("training.epochs"),
            "learning_rate": self.get("training.learning_rate"),
            "warmup_ratio": self.get("training.warmup_ratio"),
            "weight_decay": self.get("training.weight_decay"),
            "seed": self.get("training.seed"),
            "lora_r": self.get("lora.rank"),
            "lora_alpha": self.get("lora.alpha"),
            "lora_dropout": self.get("lora.dropout"),
            "max_seq_length": self.get("data.max_seq_length"),
            "per_device_train_batch_size": self.get("data.batch_size"),
            "gradient_accumulation_steps": self.get("data.gradient_accumulation"),
            "logging_steps": self.get("logging.logging_steps"),
            "save_steps": self.get("logging.save_steps"),
            "eval_steps": self.get("logging.eval_steps"),
            "gradient_checkpointing": self.get("advanced.gradient_checkpointing"),
            "no_eval": self.get("advanced.no_eval"),
            "report_to": self.get("advanced.report_to")
        }

    def get_ollama_params(self) -> Dict[str, Any]:
        """è·å– Ollama å‚æ•° (ç”¨äº Modelfile)"""
        return {
            "temperature": self.get("ollama.temperature"),
            "top_p": self.get("ollama.top_p"),
            "top_k": self.get("ollama.top_k"),
            "repeat_penalty": self.get("ollama.repeat_penalty"),
            "num_ctx": self.get("ollama.context_length")
        }

    def show_config(self):
        """æ˜¾ç¤ºå½“å‰é…ç½®"""
        print("\nğŸ“‹ å½“å‰é…ç½®:")
        print("=" * 50)
        print(f"ğŸ¤– åŸºç¡€æ¨¡å‹: {self.get('model.base_model')}")
        print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {self.get('training.epochs')}")
        print(f"ğŸ“Š å­¦ä¹ ç‡: {self.get('training.learning_rate')}")
        print(f"ğŸ”§ LoRA rank: {self.get('lora.rank')}")
        print(f"ğŸ”§ LoRA alpha: {self.get('lora.alpha')}")
        print(f"ğŸŒ¡ï¸  æ¸©åº¦: {self.get('ollama.temperature')}")
        print("=" * 50)

    def save_model_config(self, merged_dir: Path, model_name: str, actual_args: Dict = None):
        """ä¿å­˜æ¨¡å‹ä¸“ç”¨é…ç½®åˆ°æ¨¡å‹ç›®å½•"""
        model_config = {
            "model_name": model_name,
            "base_model": self.get("model.base_model"),
            "config_snapshot": self.config.copy(),
            "ollama_params": self.get_ollama_params()
        }

        # å¦‚æœæœ‰å®é™…ä½¿ç”¨çš„å‚æ•°ï¼Œä¹Ÿä¿å­˜
        if actual_args:
            model_config["actual_training_args"] = actual_args

        config_file = merged_dir / "model_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(model_config, f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ æ¨¡å‹é…ç½®å·²ä¿å­˜åˆ°: {config_file}")

def load_config(config_path: str = "config.yaml") -> ConfigManager:
    """å¿«æ·åŠ è½½é…ç½®"""
    return ConfigManager(config_path)

if __name__ == "__main__":
    # æµ‹è¯•é…ç½®ç®¡ç†å™¨
    config = ConfigManager()
    config.show_config()

    print("\nğŸ”§ è®­ç»ƒå‚æ•°:")
    training_args = config.get_training_args()
    for key, value in training_args.items():
        print(f"  {key}: {value}")