#!/usr/bin/env python3
"""
è‡ªåŠ¨å¯¼å…¥ LoRA è®­ç»ƒç»“æœåˆ° Ollama
å®Œå…¨é¿å¼€ sentencepiece ç¼–è¯‘é—®é¢˜
"""

import argparse
import json
import subprocess
import sys
from pathlib import Path
import tempfile
import shutil


def run_command(cmd: str, check: bool = True) -> tuple[int, str]:
    """è¿è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    print(f"[CMD] {cmd}")
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        if check and result.returncode != 0:
            print(f"[ERROR] å‘½ä»¤æ‰§è¡Œå¤±è´¥: {cmd}")
            print(f"[ERROR] {result.stderr}")
            return result.returncode, result.stderr
        return result.returncode, result.stdout.strip()
    except Exception as e:
        print(f"[ERROR] æ‰§è¡Œå‘½ä»¤æ—¶å‡ºé”™: {e}")
        return 1, str(e)


def check_ollama():
    """æ£€æŸ¥ Ollama æ˜¯å¦å¯ç”¨"""
    ret, _ = run_command("ollama --version", check=False)
    if ret != 0:
        print("âŒ è¯·å…ˆå®‰è£… Ollama: https://ollama.ai")
        return False

    # æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ
    ret, _ = run_command("ollama list", check=False)
    if ret != 0:
        print("ğŸ”„ å¯åŠ¨ Ollama æœåŠ¡...")
        subprocess.Popen("ollama serve", shell=True)
        import time
        time.sleep(3)

    return True


def create_modelfile(merged_dir: Path, model_name: str) -> str:
    """è‡ªåŠ¨åˆ›å»º Modelfile"""

    # è¯»å–æ¨¡å‹é…ç½®
    config_path = merged_dir / "config.json"
    config = {}
    if config_path.exists():
        with open(config_path, 'r') as f:
            config = json.load(f)

    # ç”Ÿæˆ Modelfile å†…å®¹
    modelfile_content = f"""# è‡ªåŠ¨ç”Ÿæˆçš„ Modelfile - {model_name}
# åŸºäºåˆå¹¶åçš„ HuggingFace æ¨¡å‹

FROM {merged_dir.absolute()}

# æ¨¡å‹å‚æ•°é…ç½®
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1

# ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯æ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰
SYSTEM \"\"\"ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œç»è¿‡ä¸“é—¨è®­ç»ƒä»¥æä¾›å‡†ç¡®å’Œæœ‰å¸®åŠ©çš„å›ç­”ã€‚\"\"\"
"""

    # å¦‚æœæœ‰ç‰¹å®šé…ç½®ï¼Œæ·»åŠ æ›´å¤šå‚æ•°
    if config:
        if "max_position_embeddings" in config:
            modelfile_content += f"\nPARAMETER num_ctx {config['max_position_embeddings']}"

    return modelfile_content


def import_to_ollama(merged_dir: str, model_name: str) -> bool:
    """å¯¼å…¥æ¨¡å‹åˆ° Ollama"""

    merged_path = Path(merged_dir)
    if not merged_path.exists():
        print(f"âŒ åˆå¹¶æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {merged_path}")
        return False

    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_files = ["config.json", "tokenizer.json"]
    for file in required_files:
        if not (merged_path / file).exists():
            print(f"âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶: {file}")
            return False

    # åˆ›å»ºä¸´æ—¶ Modelfile
    with tempfile.NamedTemporaryFile(mode='w', suffix='', delete=False, prefix='Modelfile_') as f:
        modelfile_content = create_modelfile(merged_path, model_name)
        f.write(modelfile_content)
        modelfile_path = f.name

    try:
        print(f"ğŸ“ ç”Ÿæˆçš„ Modelfile:")
        print("=" * 50)
        print(modelfile_content)
        print("=" * 50)

        # ä½¿ç”¨ ollama create å¯¼å…¥æ¨¡å‹
        print(f"ğŸ”„ å¯¼å…¥æ¨¡å‹åˆ° Ollama: {model_name}")
        ret, output = run_command(f"ollama create {model_name} -f {modelfile_path}")

        if ret == 0:
            print(f"âœ… æ¨¡å‹ '{model_name}' å¯¼å…¥æˆåŠŸ!")
            print(f"ğŸš€ ç°åœ¨å¯ä»¥ä½¿ç”¨: ollama run {model_name}")
            return True
        else:
            print(f"âŒ å¯¼å…¥å¤±è´¥: {output}")
            return False

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        Path(modelfile_path).unlink(missing_ok=True)


def main():
    parser = argparse.ArgumentParser(description="è‡ªåŠ¨å¯¼å…¥ LoRA è®­ç»ƒç»“æœåˆ° Ollama")
    parser.add_argument("--merged_dir", type=str, default="out/merged",
                       help="åˆå¹¶åçš„æ¨¡å‹ç›®å½•")
    parser.add_argument("--model_name", type=str, required=True,
                       help="åœ¨ Ollama ä¸­çš„æ¨¡å‹åç§°")
    parser.add_argument("--force", action="store_true",
                       help="å¼ºåˆ¶è¦†ç›–å·²å­˜åœ¨çš„æ¨¡å‹")

    args = parser.parse_args()

    print("ğŸ¯ è‡ªåŠ¨å¯¼å…¥ LoRA æ¨¡å‹åˆ° Ollama")
    print("=" * 50)

    # æ£€æŸ¥ Ollama
    if not check_ollama():
        sys.exit(1)

    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
    if not args.force:
        ret, output = run_command(f"ollama list | grep {args.model_name}", check=False)
        if ret == 0:
            print(f"âš ï¸  æ¨¡å‹ '{args.model_name}' å·²å­˜åœ¨")
            print("ğŸ’¡ ä½¿ç”¨ --force å‚æ•°å¼ºåˆ¶è¦†ç›–")
            sys.exit(1)

    # å¯¼å…¥æ¨¡å‹
    success = import_to_ollama(args.merged_dir, args.model_name)

    if success:
        print("\nğŸ‰ å¯¼å…¥å®Œæˆ!")
        print(f"ğŸ“‹ éªŒè¯æ¨¡å‹åˆ—è¡¨:")
        run_command("ollama list")

        print(f"\nğŸ”¥ æµ‹è¯•è¿è¡Œ:")
        print(f"   ollama run {args.model_name}")

    else:
        print("\nâŒ å¯¼å…¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        sys.exit(1)


if __name__ == "__main__":
    main()