# LoRA å¾®è°ƒå¿«é€Ÿä¸Šæ‰‹æŒ‡å—

> ğŸ“Œ **ç®€åŒ–ç‰ˆæœ¬**ï¼šç›´æ¥å¯ç”¨çš„å‘½ä»¤ï¼Œæ— éœ€å¤æ‚çš„è„šæœ¬

## ğŸ¯ å‰ææ¡ä»¶

- Python 3.10+
- å·²æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
- è®­ç»ƒæ•°æ®å·²å­˜åœ¨

## âš¡ æœ€ç®€æµç¨‹

### 1. è¿›å…¥ç›®å½•å¹¶æ¿€æ´»ç¯å¢ƒ

```bash
cd /Users/admin/Documents/ollama_web/finetune
source .venv/bin/activate
```

### 2. ç›´æ¥å¼€å§‹ LoRA è®­ç»ƒ

```bash
# å¿«é€Ÿæµ‹è¯•ï¼ˆ0.1 è½®ï¼Œçº¦10ç§’ï¼‰
python train_lora.py \
    --model_name_or_path "Qwen/Qwen2.5-0.5B-Instruct" \
    --output_dir "out/test" \
    --num_train_epochs 0.1 \
    --no_eval

# æ­£å¼è®­ç»ƒï¼ˆ2è½®ï¼Œçº¦5-10åˆ†é’Ÿï¼‰
python train_lora.py \
    --model_name_or_path "Qwen/Qwen2.5-0.5B-Instruct" \
    --output_dir "out/lora" \
    --num_train_epochs 2
```

### 3. åˆå¹¶æ¨¡å‹ï¼ˆå¯é€‰ï¼‰

```bash
# è®­ç»ƒå®Œæˆååˆå¹¶ LoRA åˆ°å®Œæ•´æ¨¡å‹
python merge_lora.py \
    --base_model "Qwen/Qwen2.5-0.5B-Instruct" \
    --lora_dir "out/lora" \
    --out_dir "out/merged"
```

## ğŸ“Š è®­ç»ƒç»“æœ

- **LoRA é€‚é…å™¨**: `out/lora/` (çº¦10MB)
- **å®Œæ•´æ¨¡å‹**: `out/merged/` (çº¦2GB)
- **å¯è®­ç»ƒå‚æ•°**: 4.4M / 498M (0.88%)

## ğŸ”§ å‚æ•°è¯´æ˜

### å¸¸ç”¨å‚æ•°
- `--num_train_epochs`: è®­ç»ƒè½®æ¬¡ (å»ºè®®: 0.1æµ‹è¯•, 2æ­£å¼)
- `--output_dir`: LoRA è¾“å‡ºç›®å½•
- `--no_eval`: è·³è¿‡éªŒè¯ï¼ˆåŠ é€Ÿè®­ç»ƒï¼‰
- `--learning_rate`: å­¦ä¹ ç‡ (é»˜è®¤: 2e-4)

### é«˜çº§å‚æ•°
- `--merge_and_save`: è®­ç»ƒåè‡ªåŠ¨åˆå¹¶
- `--max_seq_length`: æœ€å¤§åºåˆ—é•¿åº¦ (0=è‡ªåŠ¨)
- `--gradient_checkpointing`: èŠ‚çœæ˜¾å­˜

## ğŸš€ å®Œæ•´è®­ç»ƒ + åˆå¹¶

```bash
# ä¸€æ­¥å®Œæˆï¼šè®­ç»ƒ + åˆå¹¶
python train_lora.py \
    --model_name_or_path "Qwen/Qwen2.5-0.5B-Instruct" \
    --output_dir "out/lora" \
    --merged_dir "out/merged" \
    --num_train_epochs 2 \
    --merge_and_save
```

## ğŸ“ˆ ç³»ç»Ÿèµ„æº

### æµ‹è¯•ç¯å¢ƒ
- **è®¾å¤‡**: Apple Silicon (MPS)
- **å†…å­˜**: 24GB ç»Ÿä¸€å†…å­˜ï¼Œ8.7GB å¯ç”¨
- **ç²¾åº¦**: FP32 (MPS ç¨³å®šæ€§)
- **æ‰¹æ¬¡**: 1 æ ·æœ¬/è®¾å¤‡ï¼Œ8 æ¢¯åº¦ç´¯ç§¯

### æ€§èƒ½è¡¨ç°
- **è®­ç»ƒé€Ÿåº¦**: ~2.3ç§’/æ­¥ï¼Œ0.44æ­¥/ç§’
- **è®­ç»ƒæ—¶é—´**: 0.1è½®çº¦10ç§’ï¼Œ2è½®çº¦5-10åˆ†é’Ÿ
- **æ˜¾å­˜å ç”¨**: ç›¸æ¯”å…¨é‡å¾®è°ƒèŠ‚çœ50-80%

## âŒ å¸¸è§é—®é¢˜

### Q: è®­ç»ƒå¾ˆæ…¢ï¼Ÿ
```bash
# é™ä½åºåˆ—é•¿åº¦åŠ é€Ÿ
python train_lora.py --max_seq_length 256 --num_train_epochs 2
```

### Q: å†…å­˜ä¸å¤Ÿï¼Ÿ
```bash
# å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹
python train_lora.py --gradient_checkpointing --num_train_epochs 2
```

### Q: æƒ³å¿«é€ŸéªŒè¯ï¼Ÿ
```bash
# è¶…çŸ­è®­ç»ƒæµ‹è¯•æµç¨‹
python train_lora.py --num_train_epochs 0.01 --no_eval --output_dir "out/quick_test"
```

## ğŸ”„ ä¸å¤æ‚è„šæœ¬çš„å¯¹æ¯”

| æ–¹å¼ | ä¼˜ç‚¹ | ç¼ºç‚¹ | æ¨èåº¦ |
|------|------|------|--------|
| **ç›´æ¥å‘½ä»¤** | ç®€å•ã€å¿«é€Ÿã€å¯æ§ | éœ€è¦æ‰‹åŠ¨è®¾ç½®å‚æ•° | â­â­â­â­â­ |
| `run_mac.sh` | è‡ªåŠ¨åŒ–ç¨‹åº¦é«˜ | å¤æ‚ã€é‡å¤ç”Ÿæˆæ•°æ® | â­â­â­ |
| `OPERATION.md` | æ–‡æ¡£è¯¦ç»† | æ­¥éª¤ç¹ç | â­â­ |

## ğŸ¯ ä¸‹ä¸€æ­¥

1. **å¯¼å‡ºåˆ° Ollama**: å‚è€ƒ `export_to_ollama.md`
2. **è°ƒæ•´å‚æ•°**: æ ¹æ®æ•ˆæœè°ƒæ•´ `--num_train_epochs`
3. **å¤šè½®å®éªŒ**: å°è¯•ä¸åŒçš„ `--learning_rate`

---

**æ›´æ–°**: 2026-01-06
**æµ‹è¯•ç¯å¢ƒ**: macOS Apple Silicon MPS