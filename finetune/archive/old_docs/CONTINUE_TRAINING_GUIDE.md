# 继续训练功能使用指南

## 概述

继续训练功能让你可以在已有模型基础上进行进一步优化，支持多种训练策略：增量训练、参数调优、数据增强等。

## 🚀 快速开始

### 方法1：使用快速启动脚本
```bash
./quick_start.sh
# 选择 3) 🔄 继续训练已有模型
```

### 方法2：直接命令行
```bash
# 继续训练指定模型
python train_to_ollama.py --ollama_name "zzhua" --continue_train

# 或者直接运行，系统会自动检测并提供选项
python train_to_ollama.py --ollama_name "zzhua"
```

## 📋 继续训练选项

当检测到模型已存在时，系统提供4个选项：

### 1. 🔄 继续训练
进入继续训练模式，提供4种训练策略

### 2. 🗑️ 强制覆盖
完全重新开始训练，删除原模型

### 3. ⏭️ 跳过训练
直接使用现有模型，不进行训练

### 4. ❌ 取消操作
退出程序

## 🎯 四种继续训练策略

### 1. 📈 增量训练（推荐用于效果不够好的模型）

**适用场景：** 模型基本符合预期，但还需要更深度的学习

**原理：** 在现有训练轮次基础上增加更多训练

**示例：**
```
原始训练: 2.0 轮
增量训练: +1.5 轮
最终训练: 3.5 轮
```

**推荐设置：**
- 增加轮次：1.0-3.0 轮
- 适合数据量充足的情况

### 2. ⚙️ 调参重训（推荐用于想优化训练质量）

**适用场景：** 想通过调整参数提高训练质量

**可调整参数：**
- **学习率：**
  - 降低 (1e-4) = 更稳定但学习慢
  - 提高 (5e-4) = 学习快但可能不稳定
- **LoRA Rank：**
  - 提高 (16/32) = 更强的适应能力
  - 降低 (4) = 更轻量但能力有限

**推荐组合：**
```bash
# 高质量训练
学习率: 1e-4
LoRA rank: 16
训练轮次: 3.0

# 快速训练
学习率: 3e-4
LoRA rank: 8
训练轮次: 2.0
```

### 3. 📊 数据增强（推荐用于数据量较少）

**适用场景：** 使用当前数据集进行更深度训练

**特点：** 不改参数，只增加训练轮次

**推荐设置：**
- 小数据集 (<50条): 4.0-6.0 轮
- 中数据集 (50-200条): 3.0-4.0 轮
- 大数据集 (>200条): 2.0-3.0 轮

### 4. 🔧 自定义配置（推荐用于高级用户）

**适用场景：** 完全自定义所有参数

**可配置项：**
- 训练轮次
- 学习率
- LoRA rank (自动设置alpha = rank × 2)

## 💡 使用建议

### 根据问题选择策略

#### 模型回答质量不够好
```
选择: 2) ⚙️ 调参重训
建议: 提高LoRA rank到16，降低学习率到1e-4
```

#### 模型回答太短或不够深入
```
选择: 1) 📈 增量训练 或 3) 📊 数据增强
建议: 增加1.5-2.0轮训练
```

#### 模型偶尔出现不符合角色的回答
```
选择: 3) 📊 数据增强
建议: 使用3.0-4.0轮深度训练
```

#### 数据量很少（<20条）
```
选择: 3) 📊 数据增强
建议: 5.0-8.0轮训练 + 先添加更多数据
```

### 参数调优指南

#### 学习率选择
- **2e-4 (默认):** 平衡选择，适合大部分情况
- **1e-4:** 更稳定，适合精细调优
- **5e-4:** 更激进，适合快速实验
- **1e-5:** 非常保守，适合已接近理想状态的模型

#### LoRA Rank选择
- **4:** 超轻量，适合简单调整
- **8 (默认):** 标准配置，平衡性能和资源
- **16:** 增强能力，适合复杂角色
- **32:** 最强能力，适合非常复杂的角色和任务

## 🔍 实际使用示例

### 示例1：林栀角色优化

**现状：** 模型存在，但回答不够细腻
```bash
python train_to_ollama.py --ollama_name "zzhua" --continue_train
# 选择: 2) ⚙️ 调参重训
# 学习率: 1e-4
# LoRA rank: 16
# 训练轮次: 3.0
```

### 示例2：数据量少的快速改进

**现状：** 只有5条数据，效果不理想
```bash
python train_to_ollama.py --ollama_name "zzhua" --continue_train
# 选择: 3) 📊 数据增强
# 总训练轮次: 6.0
```

### 示例3：微调已有助手

**现状：** 通用助手想专门化
```bash
python train_to_ollama.py --ollama_name "general-assistant" --continue_train
# 选择: 1) 📈 增量训练
# 增加轮次: 2.0
```

## ⚠️ 注意事项

### 1. 备份重要模型
```bash
# 训练前备份
ollama cp original-model backup-model
```

### 2. 训练轮次建议
- **总轮次不要超过10轮**，容易过拟合
- **小数据集更需要多轮数**
- **大数据集少轮数即可**

### 3. 参数调整原则
- **一次只调整一个参数**，便于判断效果
- **先调轮次，再调学习率，最后调LoRA rank**
- **记录每次调整的效果**

### 4. 效果评估
```bash
# 训练完成后立即测试
ollama run zzhua

# 使用标准测试问题
echo "你好，请介绍一下自己" | ollama run zzhua
```

## 🚫 常见问题

### Q: 继续训练会覆盖原模型吗？
**A:** 是的，继续训练会替换原模型。建议训练前先备份。

### Q: 增量训练是从原模型继续还是重新开始？
**A:** 重新开始训练，但使用更多轮次。LoRA训练通常从基础模型开始。

### Q: 什么时候用调参重训？
**A:** 当你觉得当前参数设置不够理想，想要更高质量的训练结果时。

### Q: 数据量很少可以继续训练吗？
**A:** 可以，建议选择"数据增强"模式，使用4-8轮训练。

### Q: 继续训练需要重新准备数据吗？
**A:** 不需要，使用现有的 `data/train.jsonl` 和 `data/val.jsonl`。

## 📊 效果监控

### 训练过程监控
- 观察loss下降趋势
- 注意是否出现过拟合
- 记录训练时间

### 效果对比
```bash
# 训练前测试
echo "测试问题" | ollama run model-v1

# 训练后测试
echo "测试问题" | ollama run model-v1

# 对比回答质量
```

---

**更新时间**: 2026-01-06
**版本**: 1.0 - 继续训练功能完整指南