#!/usr/bin/env python3
"""
ä¸€é”®å¼ LoRA è®­ç»ƒ -> Ollama å¯¼å…¥
å®Œå…¨é¿å¼€ sentencepiece/llama.cpp ç¼–è¯‘é—®é¢˜
"""

import argparse
import subprocess
import sys
from pathlib import Path
import tempfile
import json
import time
import threading


def run_command(cmd: str, check: bool = True) -> tuple[int, str]:
    """è¿è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    print(f"[CMD] {cmd}")
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        if check and result.returncode != 0:
            print(f"[ERROR] {result.stderr}")
        print(result.stdout)
        return result.returncode, result.stdout.strip()
    except Exception as e:
        print(f"[ERROR] {e}")
        return 1, str(e)


def run_command_realtime(cmd: str) -> int:
    """è¿è¡Œå‘½ä»¤å¹¶å®æ—¶æ˜¾ç¤ºè¾“å‡ºï¼Œä¼˜åŒ–è¿›åº¦æ¡æ˜¾ç¤º"""
    print(f"[CMD] {cmd}")
    print("=" * 60)

    try:
        # å¯åŠ¨è¿›ç¨‹
        process = subprocess.Popen(
            cmd,
            shell=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=1
        )

        last_progress_line = ""

        # å®æ—¶è¯»å–è¾“å‡º
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break

            if output:
                line = output.strip()

                # æ£€æµ‹è¿›åº¦æ¡è¡Œï¼ˆåŒ…å« % å’Œ |ï¼‰
                if '%' in line and '|' in line and '/it]' in line:
                    # æ¸…é™¤ä¸Šä¸€è¡Œè¿›åº¦æ˜¾ç¤º
                    if last_progress_line:
                        print(f"\r{' ' * len(last_progress_line)}", end="")
                    # æ˜¾ç¤ºæ–°çš„è¿›åº¦
                    print(f"\rğŸ”„ {line}", end="", flush=True)
                    last_progress_line = f"ğŸ”„ {line}"
                else:
                    # éè¿›åº¦è¡Œï¼Œæ­£å¸¸æ˜¾ç¤º
                    if last_progress_line:
                        print()  # æ¢è¡Œ
                        last_progress_line = ""
                    print(line)

        # å¦‚æœæœ€åæœ‰è¿›åº¦è¡Œï¼Œç¡®ä¿æ¢è¡Œ
        if last_progress_line:
            print()

        # ç­‰å¾…è¿›ç¨‹å®Œæˆ
        return_code = process.poll()
        print("=" * 60)
        return return_code

    except Exception as e:
        print(f"[ERROR] {e}")
        return 1


def check_dataset():
    """æ£€æŸ¥å’Œæ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯"""
    print("ğŸ“Š æ£€æŸ¥è®­ç»ƒæ•°æ®...")

    train_file = Path("data/train.jsonl")
    val_file = Path("data/val.jsonl")

    if not train_file.exists():
        print("âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: data/train.jsonl")
        print("ğŸ’¡ è¯·è¿è¡Œ: python make_dataset.py --out_dir data --n 300")
        return False

    # ç»Ÿè®¡æ•°æ®è¡Œæ•°
    try:
        with open(train_file, 'r', encoding='utf-8') as f:
            train_count = sum(1 for _ in f)

        val_count = 0
        if val_file.exists():
            with open(val_file, 'r', encoding='utf-8') as f:
                val_count = sum(1 for _ in f)

        # è¯»å–æ•°æ®æ ·æœ¬åˆ†æ
        with open(train_file, 'r', encoding='utf-8') as f:
            sample = json.loads(f.readline())

        # åˆ†ææ•°æ®é£æ ¼
        styles = set()
        categories = set()

        with open(train_file, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= 20:  # åªè¯»å‰20è¡Œåˆ†æ
                    break
                try:
                    data = json.loads(line)
                    if 'style' in data:
                        styles.add(data['style'])
                    if 'category' in data:
                        categories.add(data['category'])
                except:
                    continue

        print(f"âœ… æ•°æ®é›†æ£€æŸ¥å®Œæˆ")
        print(f"   ğŸ“ˆ è®­ç»ƒæ•°æ®: {train_count} æ¡")
        print(f"   ğŸ“Š éªŒè¯æ•°æ®: {val_count} æ¡")
        print(f"   ğŸ“ å¯¹è¯é£æ ¼: {', '.join(sorted(styles)) if styles else 'æ ‡å‡†'}")
        print(f"   ğŸ·ï¸  æ•°æ®ç±»å‹: {', '.join(sorted(categories)) if categories else 'é€šç”¨'}")

        # æ˜¾ç¤ºæ•°æ®æ ·æœ¬
        if 'messages' in sample and sample['messages']:
            first_msg = sample['messages'][0]
            if first_msg.get('role') == 'system':
                system_prompt = first_msg.get('content', '')[:100]
                print(f"   ğŸ¯ è®­ç»ƒç›®æ ‡: {system_prompt}{'...' if len(first_msg.get('content', '')) > 100 else ''}")

        return True

    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®å¤±è´¥: {e}")
        return False


def check_environment():
    """æ£€æŸ¥ç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥ç¯å¢ƒ...")

    steps = [
        ("Pythonç‰ˆæœ¬", "python --version"),
        ("è™šæ‹Ÿç¯å¢ƒ", None),
        ("OllamaæœåŠ¡", "ollama --version"),
        ("PyTorchç¯å¢ƒ", "python -c 'import torch; print(f\"torch-{torch.__version__}\")'"),
    ]

    for step_name, cmd in steps:
        print(f"   ğŸ“‹ {step_name}...", end=" ")

        if step_name == "è™šæ‹Ÿç¯å¢ƒ":
            if hasattr(sys, 'real_prefix') or sys.base_prefix != sys.prefix:
                print("âœ… å·²æ¿€æ´»")
            else:
                print("âš ï¸  å»ºè®®ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ")
        else:
            ret, output = run_command(cmd, check=False)
            if ret == 0:
                version = output.split()[0] if output else "æ­£å¸¸"
                print(f"âœ… {version}")
            else:
                if "ollama" in cmd.lower():
                    print("âŒ è¯·å…ˆå®‰è£… Ollama")
                    return False
                else:
                    print(f"âš ï¸  {step_name}å¼‚å¸¸")

    print("âœ… ç¯å¢ƒæ£€æŸ¥å®Œæˆ")
    return True


def estimate_training_time(epochs: float, data_size: int = 300) -> str:
    """ä¼°ç®—è®­ç»ƒæ—¶é—´"""
    # åŸºäºç»éªŒï¼šæ¯ä¸ªepochå¤§çº¦2-3åˆ†é’Ÿï¼Œæ ¹æ®æ•°æ®é‡è°ƒæ•´
    base_time_per_epoch = 2.5  # åˆ†é’Ÿ
    time_factor = max(1.0, data_size / 300)  # æ•°æ®é‡è°ƒæ•´ç³»æ•°
    estimated_minutes = epochs * base_time_per_epoch * time_factor

    if estimated_minutes < 60:
        return f"çº¦ {int(estimated_minutes)} åˆ†é’Ÿ"
    else:
        hours = int(estimated_minutes // 60)
        minutes = int(estimated_minutes % 60)
        return f"çº¦ {hours} å°æ—¶ {minutes} åˆ†é’Ÿ"


def show_training_info(model_name: str, epochs: float, ollama_name: str, data_info: dict):
    """æ˜¾ç¤ºè®­ç»ƒä¿¡æ¯æ¦‚è§ˆ"""
    print("\nğŸ“‹ è®­ç»ƒä»»åŠ¡æ¦‚è§ˆ")
    print("=" * 50)
    print(f"ğŸ¤– åŸºç¡€æ¨¡å‹: {model_name}")
    print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {epochs}")
    print(f"ğŸ“¦ ç›®æ ‡æ¨¡å‹: {ollama_name}")
    print(f"ğŸ“ˆ è®­ç»ƒæ•°æ®: {data_info.get('train_count', 0)} æ¡")
    print(f"ğŸ“Š éªŒè¯æ•°æ®: {data_info.get('val_count', 0)} æ¡")
    print(f"â° é¢„è®¡æ—¶é—´: {estimate_training_time(epochs, data_info.get('train_count', 300))}")
    print("=" * 50)

    print("\nğŸ“ è®­ç»ƒæ­¥éª¤:")
    print("   1ï¸âƒ£ ç¯å¢ƒæ£€æŸ¥ âœ…")
    print("   2ï¸âƒ£ æ•°æ®éªŒè¯ âœ…")
    print("   3ï¸âƒ£ LoRAè®­ç»ƒ â³ (å®æ—¶è¿›åº¦æ˜¾ç¤º)")
    print("   4ï¸âƒ£ æ¨¡å‹åˆå¹¶ â³")
    print("   5ï¸âƒ£ å¯¼å…¥Ollama â³")


def train_lora(model_name: str, epochs: float, output_dir: str, merged_dir: str):
    """è®­ç»ƒ LoRA"""
    print(f"\nğŸš€ å¼€å§‹ LoRA å¾®è°ƒè®­ç»ƒ...")
    print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")

    cmd = f"""python train_lora.py \\
        --model_name_or_path "{model_name}" \\
        --output_dir "{output_dir}" \\
        --merged_dir "{merged_dir}" \\
        --num_train_epochs {epochs} \\
        --merge_and_save"""

    print(f"\nğŸ’¡ æç¤º: è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥çœ‹åˆ°å®æ—¶è¿›åº¦ï¼Œè¿›åº¦æ¡ä¼šåœ¨åŒä¸€è¡Œæ›´æ–°")
    print("ğŸ“Š å¦‚æœçœ‹åˆ°ç±»ä¼¼ 'ğŸ”„ 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/68 [01:37<00:16, 1.53s/it]' è¯´æ˜æ­£å¸¸è¿è¡Œ\n")

    # ä½¿ç”¨å®æ—¶æ˜¾ç¤ºåŠŸèƒ½
    start_time = time.time()
    ret = run_command_realtime(cmd)

    if ret != 0:
        print("\nâŒ è®­ç»ƒå¤±è´¥")
        print("ğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ³•:")
        print("   - æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒæ˜¯å¦æ­£ç¡®æ¿€æ´»")
        print("   - æ£€æŸ¥è®­ç»ƒæ•°æ®æ˜¯å¦å­˜åœ¨")
        print("   - æŸ¥çœ‹ä¸Šé¢çš„é”™è¯¯ä¿¡æ¯")
        return False

    elapsed_time = time.time() - start_time
    minutes = int(elapsed_time // 60)
    seconds = int(elapsed_time % 60)

    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"â° å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ• å®é™…è€—æ—¶: {minutes} åˆ† {seconds} ç§’")

    # ä¿å­˜è®­ç»ƒä¿¡æ¯åˆ°æ¨¡å‹ç›®å½•
    save_training_info(merged_dir, "è®­ç»ƒå®Œæˆ", epochs)

    return True


def save_training_info(merged_dir: str, model_name: str, epochs: float):
    """ä¿å­˜è®­ç»ƒæ—¶çš„ä¿¡æ¯åˆ°æ¨¡å‹ç›®å½•"""
    merged_path = Path(merged_dir)
    merged_path.mkdir(parents=True, exist_ok=True)

    # ä»è®­ç»ƒæ•°æ®ä¸­è¯»å–ç³»ç»Ÿæç¤ºå’Œå…¶ä»–ä¿¡æ¯
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªç»è¿‡ä¸“é—¨å¾®è°ƒçš„AIåŠ©æ‰‹ã€‚è¯·æä¾›æœ‰å¸®åŠ©ã€å‡†ç¡®å’Œå‹å¥½çš„å›ç­”ã€‚"
    training_info = {
        "model_name": model_name,
        "epochs": epochs,
        "base_model": "Qwen/Qwen2.5-0.5B-Instruct",
        "training_time": time.strftime('%Y-%m-%d %H:%M:%S')
    }

    train_file = Path("data/train.jsonl")
    if train_file.exists():
        try:
            with open(train_file, 'r', encoding='utf-8') as f:
                # è¯»å–ç¬¬ä¸€æ¡æ•°æ®è·å–ç³»ç»Ÿæç¤º
                first_line = f.readline().strip()
                if first_line:
                    data = json.loads(first_line)
                    if 'messages' in data and data['messages']:
                        for msg in data['messages']:
                            if msg.get('role') == 'system':
                                system_prompt = msg.get('content', system_prompt)
                                break
                    # ä¿å­˜è®­ç»ƒæ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
                    if 'style' in data:
                        training_info['style'] = data['style']
                    if 'category' in data:
                        training_info['category'] = data['category']
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è¯»å–è®­ç»ƒæ•°æ®ä¿¡æ¯: {e}")

    # ä¿å­˜è®­ç»ƒä¿¡æ¯åˆ°æ¨¡å‹ç›®å½•
    training_info['system_prompt'] = system_prompt
    info_path = merged_path / "training_info.json"
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(training_info, f, indent=2, ensure_ascii=False)

    print(f"ğŸ’¾ è®­ç»ƒä¿¡æ¯å·²ä¿å­˜åˆ°: {info_path}")


def create_modelfile_for_ollama(merged_dir: Path, model_name: str) -> str:
    """ä¸º Ollama åˆ›å»º Modelfile"""

    # è¯»å–ä¿å­˜çš„è®­ç»ƒä¿¡æ¯
    training_info_path = merged_dir / "training_info.json"
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªç»è¿‡ä¸“é—¨å¾®è°ƒçš„AIåŠ©æ‰‹ã€‚è¯·æä¾›æœ‰å¸®åŠ©ã€å‡†ç¡®å’Œå‹å¥½çš„å›ç­”ã€‚"  # é»˜è®¤å€¼

    if training_info_path.exists():
        try:
            with open(training_info_path, 'r', encoding='utf-8') as f:
                training_info = json.load(f)
                system_prompt = training_info.get('system_prompt', system_prompt)
                print(f"ğŸ“‹ ä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„ç³»ç»Ÿæç¤º")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è¯»å–è®­ç»ƒä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤ç³»ç»Ÿæç¤º: {e}")
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒä¿¡æ¯æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤ç³»ç»Ÿæç¤º")

    modelfile_content = f"""# LoRA å¾®è°ƒæ¨¡å‹: {model_name}
# åŸºäº Qwen2.5-0.5B-Instruct

FROM {merged_dir.absolute()}

# åŸºç¡€å‚æ•°
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.05

# ä¸Šä¸‹æ–‡é•¿åº¦
PARAMETER num_ctx 4096

# ç³»ç»Ÿæç¤º - ä»è®­ç»ƒæ—¶ä¿å­˜çš„ä¿¡æ¯ä¸­è¯»å–
SYSTEM \"\"\"{system_prompt}\"\"\"
"""

    return modelfile_content


def import_to_ollama(merged_dir: str, ollama_model_name: str) -> bool:
    """å¯¼å…¥åˆ° Ollama"""
    print(f"\nğŸ“¦ å¯¼å…¥æ¨¡å‹åˆ° Ollama: {ollama_model_name}")

    merged_path = Path(merged_dir)
    if not merged_path.exists():
        print(f"âŒ åˆå¹¶æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {merged_path}")
        return False

    # åˆ›å»ºæ ‡å‡†çš„ Modelfile
    modelfile_content = create_modelfile_for_ollama(merged_path, ollama_model_name)
    # Ollama æ ‡å‡†æ ¼å¼ï¼šå¿…é¡»å« Modelfile
    modelfile_path = merged_path / "Modelfile"

    # ä¿å­˜ Modelfile åˆ°æ¨¡å‹ç›®å½•ï¼ˆä¼šè¦†ç›–ï¼Œä½†æ¯ä¸ªæ¨¡å‹æœ‰ç‹¬ç«‹ç›®å½•ï¼‰
    with open(modelfile_path, 'w', encoding='utf-8') as f:
        f.write(modelfile_content)

    print("ğŸ“ ä½¿ç”¨çš„ Modelfile å†…å®¹:")
    print("-" * 40)
    print(modelfile_content)
    print("-" * 40)
    print(f"ğŸ’¾ Modelfile å·²ä¿å­˜åˆ°: {modelfile_path}")

    try:
        # å¯åŠ¨ Ollama æœåŠ¡ï¼ˆå¦‚æœæœªè¿è¡Œï¼‰
        print("ğŸ”„ æ£€æŸ¥ Ollama æœåŠ¡...")
        ret, _ = run_command("ollama list", check=False)
        if ret != 0:
            print("å¯åŠ¨ Ollama æœåŠ¡...")
            subprocess.Popen("ollama serve", shell=True)
            import time
            time.sleep(3)

        # å¯¼å…¥æ¨¡å‹
        ret, output = run_command(f"ollama create {ollama_model_name} -f {modelfile_path}")

        if ret == 0:
            print(f"âœ… æ¨¡å‹å¯¼å…¥æˆåŠŸ!")
            print(f"ğŸ“„ Modelfile ä½ç½®: {modelfile_path}")
            return True
        else:
            print(f"âŒ å¯¼å…¥å¤±è´¥")
            return False

    except Exception as e:
        print(f"âŒ å¯¼å…¥è¿‡ç¨‹å‡ºé”™: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(description="ä¸€é”®å¼ LoRA è®­ç»ƒåˆ° Ollama å¯¼å…¥")

    # è®­ç»ƒå‚æ•°
    parser.add_argument("--model", type=str, default="Qwen/Qwen2.5-0.5B-Instruct",
                       help="åŸºç¡€æ¨¡å‹")
    parser.add_argument("--epochs", type=float, default=2.0,
                       help="è®­ç»ƒè½®æ¬¡")
    parser.add_argument("--ollama_name", type=str, required=True,
                       help="åœ¨ Ollama ä¸­çš„æ¨¡å‹åç§°")

    # ç›®å½•å‚æ•°
    parser.add_argument("--lora_dir", type=str, default=None,
                       help="LoRA é€‚é…å™¨è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šout/lora_{æ¨¡å‹å}ï¼‰")
    parser.add_argument("--merged_dir", type=str, default=None,
                       help="åˆå¹¶æ¨¡å‹è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šout/merged_{æ¨¡å‹å}ï¼‰")

    # é€‰é¡¹
    parser.add_argument("--skip_train", action="store_true",
                       help="è·³è¿‡è®­ç»ƒï¼Œç›´æ¥å¯¼å…¥å·²æœ‰æ¨¡å‹")
    parser.add_argument("--force", action="store_true",
                       help="å¼ºåˆ¶è¦†ç›–å·²å­˜åœ¨çš„ Ollama æ¨¡å‹")

    args = parser.parse_args()

    # è‡ªåŠ¨ç”Ÿæˆæ¯ä¸ªæ¨¡å‹çš„ç‹¬ç«‹ç›®å½•
    if args.lora_dir is None:
        # æ¸…ç†æ¨¡å‹åç§°ç”¨ä½œç›®å½•å
        safe_model_name = args.ollama_name.replace(':', '_').replace('/', '_')
        args.lora_dir = f"out/lora_{safe_model_name}"

    if args.merged_dir is None:
        safe_model_name = args.ollama_name.replace(':', '_').replace('/', '_')
        args.merged_dir = f"out/merged_{safe_model_name}"

    print("ğŸ¯ ä¸€é”®å¼ LoRA è®­ç»ƒåˆ° Ollama å¯¼å…¥")
    print("=" * 50)
    print(f"ğŸ“‚ LoRA ç›®å½•: {args.lora_dir}")
    print(f"ğŸ“‚ åˆå¹¶ç›®å½•: {args.merged_dir}")
    print("=" * 50)

    # æ£€æŸ¥ç¯å¢ƒ
    if not check_environment():
        sys.exit(1)

    # æ£€æŸ¥å¹¶æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
    if not args.skip_train:
        if not check_dataset():
            sys.exit(1)

    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
    if not args.force:
        ret, _ = run_command(f"ollama list | grep {args.ollama_name}", check=False)
        if ret == 0:
            print(f"âš ï¸  æ¨¡å‹ '{args.ollama_name}' å·²å­˜åœ¨")
            print("ğŸ’¡ ä½¿ç”¨ --force å¼ºåˆ¶è¦†ç›–ï¼Œæˆ– --skip_train è·³è¿‡è®­ç»ƒ")
            sys.exit(1)

    try:
        # å‡†å¤‡æ•°æ®é›†ä¿¡æ¯ç”¨äºæ˜¾ç¤º
        data_info = {}
        if not args.skip_train:
            # è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
            train_file = Path("data/train.jsonl")
            val_file = Path("data/val.jsonl")

            if train_file.exists():
                with open(train_file, 'r', encoding='utf-8') as f:
                    data_info['train_count'] = sum(1 for _ in f)

            if val_file.exists():
                with open(val_file, 'r', encoding='utf-8') as f:
                    data_info['val_count'] = sum(1 for _ in f)

        # æ˜¾ç¤ºè®­ç»ƒæ¦‚è§ˆ
        if not args.skip_train:
            show_training_info(args.model, args.epochs, args.ollama_name, data_info)

        # æ­¥éª¤ 1: è®­ç»ƒï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not args.skip_train:
            success = train_lora(
                model_name=args.model,
                epochs=args.epochs,
                output_dir=args.lora_dir,
                merged_dir=args.merged_dir
            )
            if not success:
                sys.exit(1)
        else:
            print("â­ï¸  è·³è¿‡è®­ç»ƒï¼Œä½¿ç”¨ç°æœ‰æ¨¡å‹")

        # æ­¥éª¤ 2: å¯¼å…¥åˆ° Ollama
        success = import_to_ollama(args.merged_dir, args.ollama_name)

        if success:
            print(f"\nğŸ‰ å®Œæˆ! æ¨¡å‹å·²å¯¼å…¥ä¸º: {args.ollama_name}")
            print("\nğŸ“‹ éªŒè¯:")
            run_command("ollama list")
            print(f"\nğŸš€ æµ‹è¯•è¿è¡Œ:")
            print(f"   ollama run {args.ollama_name}")
            print(f"\nğŸ’¡ æç¤º: å³ä½¿åˆ é™¤åŸå§‹ Qwen æ¨¡å‹ï¼Œ{args.ollama_name} ä¹Ÿä¼šç‹¬ç«‹å­˜åœ¨")

        else:
            print("âŒ å¯¼å…¥å¤±è´¥")
            sys.exit(1)

    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()