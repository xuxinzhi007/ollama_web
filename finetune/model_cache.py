#!/usr/bin/env python3
"""
æ¨¡å‹ç¼“å­˜æ£€æµ‹å’Œç®¡ç†å·¥å…·
è§£å†³é‡å¤ä¸‹è½½æ¨¡å‹çš„é—®é¢˜
"""

import os
from pathlib import Path
from typing import Optional, Tuple
import tempfile

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from transformers.utils import TRANSFORMERS_CACHE
except ImportError:
    print("âš ï¸ transformersåº“æœªå®‰è£…ï¼Œæ— æ³•æ£€æµ‹æ¨¡å‹ç¼“å­˜")
    TRANSFORMERS_CACHE = None


def get_cache_dir() -> Path:
    """è·å–HuggingFaceç¼“å­˜ç›®å½•"""
    if TRANSFORMERS_CACHE:
        return Path(TRANSFORMERS_CACHE)

    # å›é€€åˆ°é»˜è®¤ä½ç½®
    home = Path.home()
    return home / ".cache" / "huggingface" / "transformers"


def is_model_cached(model_name: str) -> Tuple[bool, Optional[Path]]:
    """
    æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»ç¼“å­˜

    Args:
        model_name: æ¨¡å‹åç§°ï¼Œå¦‚ "Qwen/Qwen2.5-0.5B-Instruct"

    Returns:
        (æ˜¯å¦ç¼“å­˜, ç¼“å­˜è·¯å¾„)
    """
    try:
        # æ–¹æ³•1: å°è¯•åŠ è½½tokenizeré…ç½®æ–‡ä»¶ï¼Œä¸å®é™…ä¸‹è½½
        from transformers import AutoConfig

        # ä½¿ç”¨ä¸´æ—¶ç›®å½•ï¼Œlocal_files_only=True å¼ºåˆ¶åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        config = AutoConfig.from_pretrained(
            model_name,
            local_files_only=True,
            cache_dir=None  # ä½¿ç”¨é»˜è®¤ç¼“å­˜
        )

        # å¦‚æœèƒ½æˆåŠŸåŠ è½½é…ç½®ï¼Œè¯´æ˜æ¨¡å‹å·²ç¼“å­˜
        cache_dir = get_cache_dir()

        # æŸ¥æ‰¾å®é™…çš„æ¨¡å‹æ–‡ä»¶å¤¹
        model_hash_dirs = list(cache_dir.glob(f"*{model_name.replace('/', '--')}*"))
        if model_hash_dirs:
            return True, model_hash_dirs[0]

        return True, cache_dir  # é…ç½®èƒ½åŠ è½½ä½†æ‰¾ä¸åˆ°å…·ä½“è·¯å¾„

    except Exception:
        # å¦‚æœåŠ è½½å¤±è´¥ï¼Œè¯´æ˜æ¨¡å‹æœªç¼“å­˜æˆ–ç¼“å­˜ä¸å®Œæ•´
        return False, None


def check_model_files(model_name: str) -> dict:
    """
    è¯¦ç»†æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ç¼“å­˜çŠ¶æ€

    Returns:
        {
            'cached': bool,
            'cache_path': str,
            'files': {
                'config': bool,
                'tokenizer': bool,
                'model': bool,
            },
            'size': str
        }
    """
    result = {
        'cached': False,
        'cache_path': None,
        'files': {
            'config': False,
            'tokenizer': False,
            'model': False,
        },
        'size': '0 MB'
    }

    try:
        is_cached, cache_path = is_model_cached(model_name)
        if not is_cached:
            return result

        result['cached'] = True
        result['cache_path'] = str(cache_path) if cache_path else "æœªçŸ¥"

        # æ£€æŸ¥å…³é”®æ–‡ä»¶
        cache_dir = get_cache_dir()

        # æŸ¥æ‰¾æ¨¡å‹ç›¸å…³çš„æ–‡ä»¶
        model_pattern = model_name.replace('/', '--')
        model_files = list(cache_dir.glob(f"*{model_pattern}*"))

        total_size = 0
        config_found = False
        tokenizer_found = False
        model_found = False

        for file_path in cache_dir.rglob("*"):
            if not file_path.is_file():
                continue

            if model_pattern in str(file_path):
                file_size = file_path.stat().st_size
                total_size += file_size

                filename = file_path.name.lower()
                if "config" in filename:
                    config_found = True
                elif "tokenizer" in filename:
                    tokenizer_found = True
                elif any(x in filename for x in ["pytorch_model", "model.safetensors", ".bin"]):
                    model_found = True

        result['files']['config'] = config_found
        result['files']['tokenizer'] = tokenizer_found
        result['files']['model'] = model_found

        # è½¬æ¢æ–‡ä»¶å¤§å°
        if total_size > 0:
            if total_size > 1024 * 1024 * 1024:  # > 1GB
                result['size'] = f"{total_size / (1024**3):.1f} GB"
            else:  # MB
                result['size'] = f"{total_size / (1024**2):.1f} MB"

    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥æ¨¡å‹ç¼“å­˜æ—¶å‡ºé”™: {e}")

    return result


def print_cache_status(model_name: str):
    """æ‰“å°æ¨¡å‹ç¼“å­˜çŠ¶æ€"""
    print(f"\nğŸ” æ£€æŸ¥æ¨¡å‹ç¼“å­˜: {model_name}")

    status = check_model_files(model_name)

    if status['cached']:
        print("âœ… æ¨¡å‹å·²ç¼“å­˜")
        print(f"   ğŸ“ ç¼“å­˜è·¯å¾„: {status['cache_path']}")
        print(f"   ğŸ“¦ ç¼“å­˜å¤§å°: {status['size']}")
        print(f"   ğŸ“„ é…ç½®æ–‡ä»¶: {'âœ…' if status['files']['config'] else 'âŒ'}")
        print(f"   ğŸ”¤ åˆ†è¯å™¨: {'âœ…' if status['files']['tokenizer'] else 'âŒ'}")
        print(f"   ğŸ¤– æ¨¡å‹æƒé‡: {'âœ…' if status['files']['model'] else 'âŒ'}")

        # æ£€æŸ¥ç¼“å­˜å®Œæ•´æ€§
        files = status['files']
        if all([files['config'], files['tokenizer'], files['model']]):
            print("ğŸ‰ ç¼“å­˜å®Œæ•´ï¼Œå°†ä½¿ç”¨æœ¬åœ°æ–‡ä»¶")
            return True
        else:
            print("âš ï¸ ç¼“å­˜ä¸å®Œæ•´ï¼Œå¯èƒ½éœ€è¦é‡æ–°ä¸‹è½½éƒ¨åˆ†æ–‡ä»¶")
            return False
    else:
        print("âŒ æ¨¡å‹æœªç¼“å­˜")
        print("ğŸ’¡ é¦–æ¬¡ä½¿ç”¨éœ€è¦ä»ç½‘ç»œä¸‹è½½ï¼ˆçº¦500MB-1GBï¼‰")
        return False


def estimate_download_time(model_name: str) -> str:
    """ä¼°ç®—ä¸‹è½½æ—¶é—´"""
    # æ ¹æ®æ¨¡å‹å¤§å°ä¼°ç®—
    if "0.5B" in model_name:
        return "çº¦1-3åˆ†é’Ÿ"
    elif "1.5B" in model_name or "1B" in model_name:
        return "çº¦3-5åˆ†é’Ÿ"
    elif "7B" in model_name:
        return "çº¦10-15åˆ†é’Ÿ"
    else:
        return "å‡ åˆ†é’Ÿ"


def smart_model_load_message(model_name: str):
    """æ™ºèƒ½æ˜¾ç¤ºæ¨¡å‹åŠ è½½æ¶ˆæ¯"""
    print(f"\nğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")

    if print_cache_status(model_name):
        print("âš¡ ä½¿ç”¨ç¼“å­˜ï¼ŒåŠ è½½é€Ÿåº¦æ›´å¿«")
    else:
        download_time = estimate_download_time(model_name)
        print(f"ğŸŒ ä»ç½‘ç»œä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œé¢„è®¡è€—æ—¶: {download_time}")


if __name__ == "__main__":
    # æµ‹è¯•å¸¸è§æ¨¡å‹
    models = [
        "Qwen/Qwen2.5-0.5B-Instruct",
        "Qwen/Qwen2.5-1.5B-Instruct",
    ]

    for model in models:
        print_cache_status(model)
        print()