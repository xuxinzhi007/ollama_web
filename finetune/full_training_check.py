#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨é¡¹ç›®è®­ç»ƒæ£€æŸ¥ - æ£€æŸ¥è®­ç»ƒæ˜¯å¦çœŸçš„æ‰§è¡Œ
"""

import json
import sys
import io
from pathlib import Path
from datetime import datetime

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

def check_training_status():
    """å…¨é¢æ£€æŸ¥è®­ç»ƒçŠ¶æ€"""
    
    print("=" * 70)
    print("å…¨é¡¹ç›®è®­ç»ƒæ£€æŸ¥")
    print("=" * 70)
    
    character = "linzhi"
    
    # 1. æ£€æŸ¥è®­ç»ƒè¾“å‡ºç›®å½•
    print("\nğŸ“ 1. æ£€æŸ¥è®­ç»ƒè¾“å‡ºç›®å½•")
    print("-" * 70)
    
    lora_dir = Path(f"out/lora_{character}")
    merged_dir = Path(f"out/merged_{character}")
    
    lora_exists = lora_dir.exists()
    merged_exists = merged_dir.exists()
    
    print(f"  LoRAç›®å½•: {lora_dir} - {'âœ… å­˜åœ¨' if lora_exists else 'âŒ ä¸å­˜åœ¨'}")
    print(f"  åˆå¹¶ç›®å½•: {merged_dir} - {'âœ… å­˜åœ¨' if merged_exists else 'âŒ ä¸å­˜åœ¨'}")
    
    if lora_exists:
        files = list(lora_dir.glob("*"))
        print(f"  æ–‡ä»¶æ•°é‡: {len(files)}")
        for f in files[:10]:
            size = f.stat().st_size / 1024 / 1024 if f.is_file() else 0
            mtime = datetime.fromtimestamp(f.stat().st_mtime)
            print(f"    - {f.name} ({size:.1f}MB, {mtime.strftime('%Y-%m-%d %H:%M')})")
    
    # 2. æ£€æŸ¥checkpoint
    print("\nğŸ“Š 2. æ£€æŸ¥è®­ç»ƒcheckpoint")
    print("-" * 70)
    
    if lora_exists:
        checkpoints = sorted([d for d in lora_dir.iterdir() if d.is_dir() and d.name.startswith('checkpoint-')])
        print(f"  Checkpointæ•°é‡: {len(checkpoints)}")
        
        if checkpoints:
            latest = checkpoints[-1]
            print(f"  æœ€æ–°checkpoint: {latest.name}")
            
            # æ£€æŸ¥trainer_state.json
            trainer_state = latest / "trainer_state.json"
            if trainer_state.exists():
                try:
                    with open(trainer_state, 'r', encoding='utf-8') as f:
                        state = json.load(f)
                    
                    print(f"  âœ… è®­ç»ƒçŠ¶æ€æ–‡ä»¶å­˜åœ¨")
                    print(f"     Epoch: {state.get('epoch', 'N/A')}")
                    print(f"     æ€»æ­¥æ•°: {state.get('max_steps', 'N/A')}")
                    print(f"     å·²å®Œæˆæ­¥æ•°: {state.get('global_step', 'N/A')}")
                    print(f"     æœ€æ–°loss: {state.get('log_history', [{}])[-1].get('loss', 'N/A') if state.get('log_history') else 'N/A'}")
                    
                except Exception as e:
                    print(f"  âš ï¸  æ— æ³•è¯»å–è®­ç»ƒçŠ¶æ€: {e}")
            else:
                print(f"  âŒ è®­ç»ƒçŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨")
            
            # æ£€æŸ¥LoRAæƒé‡æ–‡ä»¶
            adapter_files = list(latest.glob("adapter_model.*"))
            print(f"  LoRAæƒé‡æ–‡ä»¶: {len(adapter_files)}ä¸ª")
            for f in adapter_files:
                size = f.stat().st_size / 1024 / 1024
                print(f"    - {f.name} ({size:.1f}MB)")
        else:
            print(f"  âŒ æ²¡æœ‰æ‰¾åˆ°checkpoint")
    else:
        print(f"  âŒ LoRAç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•æ£€æŸ¥checkpoint")
    
    # 3. æ£€æŸ¥åˆå¹¶æ¨¡å‹
    print("\nğŸ”— 3. æ£€æŸ¥åˆå¹¶æ¨¡å‹")
    print("-" * 70)
    
    if merged_exists:
        model_files = list(merged_dir.glob("*.safetensors")) + list(merged_dir.glob("*.bin"))
        config_files = list(merged_dir.glob("config.json"))
        tokenizer_files = list(merged_dir.glob("tokenizer*"))
        
        print(f"  æ¨¡å‹æ–‡ä»¶: {len(model_files)}ä¸ª")
        for f in model_files[:5]:
            size = f.stat().st_size / 1024 / 1024
            print(f"    - {f.name} ({size:.1f}MB)")
        
        print(f"  é…ç½®æ–‡ä»¶: {len(config_files)}ä¸ª")
        print(f"  Tokenizeræ–‡ä»¶: {len(tokenizer_files)}ä¸ª")
        
        if model_files and config_files and tokenizer_files:
            print(f"  âœ… åˆå¹¶æ¨¡å‹æ–‡ä»¶å®Œæ•´")
        else:
            print(f"  âš ï¸  åˆå¹¶æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´")
    else:
        print(f"  âŒ åˆå¹¶æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
    
    # 4. æ£€æŸ¥Ollamaæ¨¡å‹
    print("\nğŸ¤– 4. æ£€æŸ¥Ollamaæ¨¡å‹")
    print("-" * 70)
    
    try:
        import subprocess
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            output = result.stdout
            if 'linzhi-lora' in output:
                print(f"  âœ… Ollamaæ¨¡å‹å­˜åœ¨: linzhi-lora")
                
                # æ£€æŸ¥æ¨¡å‹ä¿¡æ¯
                info_result = subprocess.run(['ollama', 'show', 'linzhi-lora'], capture_output=True, text=True, timeout=5)
                if info_result.returncode == 0:
                    print(f"  æ¨¡å‹ä¿¡æ¯:")
                    for line in info_result.stdout.split('\n')[:10]:
                        if line.strip():
                            print(f"    {line}")
            else:
                print(f"  âŒ Ollamaæ¨¡å‹ä¸å­˜åœ¨: linzhi-lora")
        else:
            print(f"  âš ï¸  æ— æ³•æ£€æŸ¥Ollamaæ¨¡å‹ï¼ˆollamaå‘½ä»¤å¤±è´¥ï¼‰")
    except Exception as e:
        print(f"  âš ï¸  æ— æ³•æ£€æŸ¥Ollamaæ¨¡å‹: {e}")
    
    # 5. æ£€æŸ¥è®­ç»ƒæ•°æ®
    print("\nğŸ“š 5. æ£€æŸ¥è®­ç»ƒæ•°æ®")
    print("-" * 70)
    
    train_file = Path(f"datasets/{character}/train.jsonl")
    if train_file.exists():
        with open(train_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        print(f"  âœ… è®­ç»ƒæ–‡ä»¶å­˜åœ¨: {train_file}")
        print(f"  æ ·æœ¬æ•°é‡: {len(lines)}")
        
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬
        if lines:
            try:
                data = json.loads(lines[0])
                messages = data.get('messages', [])
                print(f"  ç¬¬ä¸€ä¸ªæ ·æœ¬æ¶ˆæ¯æ•°: {len(messages)}")
                print(f"  æ¶ˆæ¯è§’è‰²: {[m.get('role') for m in messages]}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰system
                has_system = any(m.get('role') == 'system' for m in messages)
                print(f"  åŒ…å«system: {'âŒ æ˜¯' if has_system else 'âœ… å¦'}")
                
            except Exception as e:
                print(f"  âš ï¸  æ— æ³•è§£æç¬¬ä¸€ä¸ªæ ·æœ¬: {e}")
    else:
        print(f"  âŒ è®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨: {train_file}")
    
    # 6. æ£€æŸ¥è®­ç»ƒé…ç½®
    print("\nâš™ï¸  6. æ£€æŸ¥è®­ç»ƒé…ç½®")
    print("-" * 70)
    
    try:
        import yaml
        with open("character_configs.yaml", 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        char_config = config.get('characters', {}).get(character, {})
        training_params = char_config.get('training_params', {})
        
        print(f"  âœ… é…ç½®æ–‡ä»¶å­˜åœ¨")
        print(f"  Epochs: {training_params.get('epochs', 'N/A')}")
        print(f"  Learning rate: {training_params.get('learning_rate', 'N/A')}")
        print(f"  LoRA r: {training_params.get('lora_r', 'N/A')}")
        print(f"  Base model: {training_params.get('base_model', 'N/A')}")
        
    except Exception as e:
        print(f"  âš ï¸  æ— æ³•è¯»å–é…ç½®: {e}")
    
    # 7. æ£€æŸ¥è®­ç»ƒæ—¥å¿—ï¼ˆå¦‚æœæœ‰ï¼‰
    print("\nğŸ“ 7. æ£€æŸ¥è®­ç»ƒæ—¥å¿—")
    print("-" * 70)
    
    log_files = list(Path(".").glob("*.log")) + list(Path(".").glob("training_*.txt"))
    if log_files:
        print(f"  æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶: {len(log_files)}ä¸ª")
        for f in log_files[:5]:
            size = f.stat().st_size / 1024
            mtime = datetime.fromtimestamp(f.stat().st_mtime)
            print(f"    - {f.name} ({size:.1f}KB, {mtime.strftime('%Y-%m-%d %H:%M')})")
    else:
        print(f"  â„¹ï¸  æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
    
    # 8. æ€»ç»“å’Œå»ºè®®
    print("\nğŸ’¡ 8. æ€»ç»“å’Œå»ºè®®")
    print("-" * 70)
    
    issues = []
    
    if not lora_exists:
        issues.append("âŒ LoRAç›®å½•ä¸å­˜åœ¨ - å¯èƒ½æ²¡æœ‰è®­ç»ƒè¿‡")
    
    if lora_exists and not checkpoints:
        issues.append("âŒ æ²¡æœ‰checkpoint - è®­ç»ƒå¯èƒ½æ²¡æœ‰å®Œæˆ")
    
    if not merged_exists:
        issues.append("âŒ åˆå¹¶æ¨¡å‹ä¸å­˜åœ¨ - æ— æ³•å¯¼å…¥Ollama")
    
    if not issues:
        print("  âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡")
        print("  ğŸ’¡ å¦‚æœæ¨¡å‹æ•ˆæœä¸å¥½ï¼Œå¯èƒ½æ˜¯:")
        print("     1. è®­ç»ƒæ•°æ®è´¨é‡é—®é¢˜")
        print("     2. è®­ç»ƒå‚æ•°ä¸åˆé€‚")
        print("     3. æ¨¡å‹è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆ")
        print("     4. Ollamaå¯¼å…¥æ—¶ä½¿ç”¨äº†é”™è¯¯çš„æ¨¡å‹")
    else:
        print("  âš ï¸  å‘ç°ä»¥ä¸‹é—®é¢˜:")
        for issue in issues:
            print(f"     {issue}")
        print("\n  ğŸ”§ å»ºè®®:")
        print("     1. è¿è¡Œ .\\quick_fix.ps1 æ¸…ç†æ—§æ¨¡å‹")
        print("     2. è¿è¡Œ .\\train.ps1 linzhi é‡æ–°è®­ç»ƒ")
        print("     3. ç¡®ä¿è®­ç»ƒå®Œæˆåå†å¯¼å…¥Ollama")

if __name__ == "__main__":
    check_training_status()

