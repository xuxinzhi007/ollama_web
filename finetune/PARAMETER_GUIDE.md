# 📋 LoRA训练参数调整指南

## 🎯 训练参数 (training_params)

### 核心参数说明

| 参数 | 作用 | 推荐范围 | 调整建议 |
|------|------|----------|----------|
| `epochs` | 训练轮数 | 2-4 | 数据多用2-3轮，数据少用3-4轮，超过5轮容易过拟合 |
| `learning_rate` | 学习率 | 1e-5 ~ 1e-4 | 大模型用低值(2e-5)，小模型可用高值(1e-4) |
| `lora_r` | LoRA rank | 8-32 | 复杂角色用16-32，简单角色用8-16 |
| `lora_alpha` | LoRA缩放 | rank×2 | 通常设为rank的2倍，影响适配强度 |
| `lora_dropout` | 防过拟合 | 0.05-0.15 | 数据少用0.1-0.15，数据多用0.05-0.1 |

### 常见问题诊断

**🔴 过拟合症状**：
- 训练loss很低，但对话质量差
- 回复过于机械，缺乏自然性
- 严重偏离角色设定

**解决方案**：
- 减少 `epochs` (降到2-3轮)
- 增加 `lora_dropout` (提升到0.1-0.15)
- 降低 `lora_r` (减少到8-16)

**🟡 欠拟合症状**：
- 训练后角色特征不明显
- 回复太通用，没有个性
- 训练loss还在下降但停止了

**解决方案**：
- 增加 `epochs` (提升到3-4轮)
- 提高 `learning_rate` (从2e-5提升到5e-5)
- 增加 `lora_r` (提升到16-32)

## 🎭 推理参数 (inference_params)

### 核心参数说明

| 参数 | 作用 | 推荐范围 | 不同角色的典型值 |
|------|------|----------|-----------------|
| `temperature` | 创造性vs稳定性 | 0.3-0.8 | 严肃角色:0.3-0.5，活泼角色:0.6-0.8 |
| `top_p` | 词汇丰富度 | 0.8-0.95 | 简洁风格:0.8-0.85，丰富表达:0.9-0.95 |
| `top_k` | 候选词数量 | 20-80 | 专业领域:20-40，日常对话:40-80 |
| `repeat_penalty` | 重复惩罚 | 1.05-1.15 | 避免重复:1.1-1.15，自然表达:1.05-1.08 |
| `num_predict` | 回复长度 | 200-800 | 简短回复:200-300，详细回复:400-800 |

### 不同角色类型的推荐配置

**🌸 温柔角色** (如林栀)：
```yaml
temperature: 0.65      # 温和自然
top_p: 0.92           # 丰富表达
repeat_penalty: 1.08   # 轻度避免重复
num_predict: 420       # 适中长度
```

**⚡ 活泼角色**：
```yaml
temperature: 0.75      # 更有创意
top_p: 0.95           # 最大词汇范围
repeat_penalty: 1.05   # 允许一定重复
num_predict: 300       # 简洁有力
```

**🎓 专业角色**：
```yaml
temperature: 0.4       # 稳定准确
top_p: 0.85           # 专业词汇
repeat_penalty: 1.12   # 避免术语重复
num_predict: 600       # 详细说明
```

## 🔧 快速调试流程

### 1. 基础验证
```bash
# 使用快速版本测试配置
python smart_train.py linzhi_quick
```

### 2. 效果不好时的调整步骤

**Step 1: 检查训练效果**
- 训练loss是否正常下降
- 是否出现过拟合迹象

**Step 2: 调整训练参数**
```yaml
# 保守调整（推荐）
epochs: 3.0 → 2.5        # 减少过拟合
learning_rate: 5e-5 → 3e-5  # 更稳定学习
```

**Step 3: 优化推理表现**
```yaml
# 如果回复太机械
temperature: 0.65 → 0.7   # 增加自然性

# 如果经常重复
repeat_penalty: 1.08 → 1.12  # 增强惩罚
```

### 3. 验证最终效果
```bash
# 导入测试
python train_to_ollama.py --character linzhi --ollama_name test-model

# 对话测试
ollama run test-model
```

## 📊 性能优化建议

### 训练速度优化
- 使用较小的 `lora_r` (8-16)
- 减少 `epochs` 到必要的最小值
- 使用快速版本进行参数调试

### 显存优化
- 降低 `lora_r` 减少参数量
- 启用 `gradient_checkpointing`
- 使用较小的batch_size

### 质量优化
- 保证数据质量 > 数据数量
- 合理设置 `lora_dropout` 防止过拟合
- 针对角色特点调整推理参数

## 🎯 总结

**训练阶段**：重点关注防止过拟合，保持训练稳定性
**推理阶段**：根据角色特点个性化调整，平衡自然性和一致性

记住：**小步调整，频繁测试** 比大幅度修改更安全有效！